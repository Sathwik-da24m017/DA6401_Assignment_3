{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/sathwikpentela/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda24m017\u001b[0m (\u001b[33mda24m017-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key='f56388c51b488c425a228537fd2d35e5498a3a91')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:22:14.170759Z",
     "iopub.status.busy": "2025-05-20T15:22:14.170464Z",
     "iopub.status.idle": "2025-05-20T15:22:18.023546Z",
     "shell.execute_reply": "2025-05-20T15:22:18.022740Z",
     "shell.execute_reply.started": "2025-05-20T15:22:14.170725Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, cell_type=\"LSTM\"):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        rnn_cls = {\n",
    "            \"RNN\": nn.RNN,\n",
    "            \"GRU\": nn.GRU,\n",
    "            \"LSTM\": nn.LSTM\n",
    "        }[cell_type]\n",
    "\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "        outputs, hidden = self.rnn(embedded)  # outputs: all hidden states\n",
    "        return hidden  # hidden: (num_layers, batch, hidden_size)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, cell_type=\"LSTM\"):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        rnn_cls = {\n",
    "            \"RNN\": nn.RNN,\n",
    "            \"GRU\": nn.GRU,\n",
    "            \"LSTM\": nn.LSTM\n",
    "        }[cell_type]\n",
    "\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # x: [batch_size] -> unsqueezed to [batch_size, 1]\n",
    "        x = x.unsqueeze(1)\n",
    "        embedded = self.embedding(x)  # [batch_size, 1, embedding_dim]\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        prediction = self.fc_out(output.squeeze(1))  # [batch_size, vocab_size]\n",
    "        return prediction, hidden\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src: [batch_size, src_len]\n",
    "        # trg: [batch_size, trg_len]\n",
    "        batch_size = trg.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n",
    "\n",
    "        hidden = self.encoder(src)\n",
    "\n",
    "        # handle LSTM's (hidden, cell) separately\n",
    "        input = trg[:, 0]  # first input to decoder is <sos> token\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Question 1(a): Total Number of Computations\n",
    "\n",
    "We analyze the total number of computations performed by the RNN-based seq2seq model, under the following assumptions:\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Assumptions\n",
    "\n",
    "Let:\n",
    "- \\( T \\): Length of input/output sequences\n",
    "- \\( m \\): Input embedding size\n",
    "- \\( k \\): Hidden state size\n",
    "- \\( V \\): Vocabulary size (same for both languages)\n",
    "- Encoder and decoder have 1 layer each\n",
    "- We use any RNN cell (Vanilla RNN / GRU / LSTM)\n",
    "- Input and output sequence lengths are equal\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Encoder RNN\n",
    "\n",
    "Each time step performs the following:\n",
    "- Multiply input \\( x_t \\in \\mathbb{R}^m \\) with weights \\( W_{ih} \\in \\mathbb{R}^{k \\times m} \\):  \n",
    "  ‚Üí \\( O(km) \\)\n",
    "- Multiply hidden state \\( h_{t-1} \\in \\mathbb{R}^k \\) with \\( W_{hh} \\in \\mathbb{R}^{k \\times k} \\):  \n",
    "  ‚Üí \\( O(k^2) \\)\n",
    "\n",
    "Ignoring biases and activation costs:\n",
    "- **Per time step cost**: \\( O(km + k^2) \\)\n",
    "- **Total for T steps**:  \n",
    "  \\[\n",
    "  O(T(km + k^2))\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ Decoder RNN\n",
    "\n",
    "The decoder runs for \\( T \\) steps as well:\n",
    "- Same per-step cost as encoder\n",
    "- **Total for T steps**:  \n",
    "  \\[\n",
    "  O(T(km + k^2))\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Output Projection Layer\n",
    "\n",
    "Each decoder step outputs a probability over the vocabulary:\n",
    "- Multiply \\( h_t \\in \\mathbb{R}^k \\) with \\( W_o \\in \\mathbb{R}^{V \\times k} \\):  \n",
    "  ‚Üí \\( O(Vk) \\)\n",
    "\n",
    "Total for \\( T \\) steps:  \n",
    "\\[\n",
    "O(TVk)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Final Total Computation\n",
    "\n",
    "\\[\n",
    "\\text{Encoder: } O(T(km + k^2)) \\\\\n",
    "\\text{Decoder: } O(T(km + k^2)) \\\\\n",
    "\\text{Projection: } O(TVk)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\Rightarrow \\boxed{O(T(2km + 2k^2 + Vk))}\n",
    "\\]\n",
    "\n",
    "This is the total number of computations performed during a forward pass of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Question 1(b): Total Number of Parameters\n",
    "\n",
    "We now compute the total number of trainable parameters in the RNN-based seq2seq model, using the following assumptions:\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Assumptions\n",
    "\n",
    "Let:\n",
    "- \\( V \\): Vocabulary size (same for source and target)\n",
    "- \\( m \\): Input embedding size\n",
    "- \\( k \\): Hidden state size\n",
    "- Encoder and decoder have 1 layer each\n",
    "- Same input/output vocabulary sizes\n",
    "- Biases are included\n",
    "\n",
    "---\n",
    "\n",
    "### üî∑ Embedding Layers\n",
    "\n",
    "- **Source embedding**: \\( V \\times m \\)\n",
    "- **Target embedding**: \\( V \\times m \\)\n",
    "\n",
    "\\[\n",
    "\\Rightarrow \\textbf{Total = } \\boxed{2Vm}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Encoder RNN Parameters\n",
    "\n",
    "For a vanilla RNN cell:\n",
    "- Input weights: \\( W_{ih} \\in \\mathbb{R}^{k \\times m} \\Rightarrow km \\)\n",
    "- Recurrent weights: \\( W_{hh} \\in \\mathbb{R}^{k \\times k} \\Rightarrow k^2 \\)\n",
    "- Bias: \\( b \\in \\mathbb{R}^{k} \\Rightarrow k \\)\n",
    "\n",
    "\\[\n",
    "\\Rightarrow \\textbf{Total = } \\boxed{km + k^2 + k}\n",
    "\\]\n",
    "\n",
    "(Note: GRU and LSTM have more parameters, but we're assuming a basic RNN cell for now.)\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ Decoder RNN Parameters\n",
    "\n",
    "Same as encoder:\n",
    "\n",
    "\\[\n",
    "\\Rightarrow \\textbf{Total = } \\boxed{km + k^2 + k}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Output Projection Layer\n",
    "\n",
    "- Weights: \\( W_o \\in \\mathbb{R}^{V \\times k} \\Rightarrow Vk \\)\n",
    "- Bias: \\( b_o \\in \\mathbb{R}^{V} \\Rightarrow V \\)\n",
    "\n",
    "\\[\n",
    "\\Rightarrow \\textbf{Total = } \\boxed{Vk + V}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Final Parameter Count\n",
    "\n",
    "\\[\n",
    "\\text{Embeddings: } 2Vm \\\\\n",
    "\\text{Encoder RNN: } km + k^2 + k \\\\\n",
    "\\text{Decoder RNN: } km + k^2 + k \\\\\n",
    "\\text{Output Layer: } Vk + V\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\Rightarrow \\boxed{2Vm + 2km + 2k^2 + 2k + Vk + V}\n",
    "\\]\n",
    "\n",
    "This is the total number of parameters in the network when using a 1-layer RNN encoder and decoder with matching vocabulary and hidden dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:22:20.696948Z",
     "iopub.status.busy": "2025-05-20T15:22:20.696484Z",
     "iopub.status.idle": "2025-05-20T15:22:20.883006Z",
     "shell.execute_reply": "2025-05-20T15:22:20.882224Z",
     "shell.execute_reply.started": "2025-05-20T15:22:20.696923Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample (roman -> native): ('amkita', '‡∞Ö‡∞Ç‡∞ï‡∞ø‡∞§')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# File paths\n",
    "BASE_PATH = Path(\"dakshina_dataset_v1.0/te/lexicons/\")\n",
    "TRAIN_FILE = BASE_PATH / \"te.translit.sampled.train.tsv\"\n",
    "DEV_FILE = BASE_PATH / \"te.translit.sampled.dev.tsv\"\n",
    "TEST_FILE = BASE_PATH / \"te.translit.sampled.test.tsv\"\n",
    "\n",
    "# Load data\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"target\", \"input\", \"attestations\"])\n",
    "    data.dropna(subset=[\"input\", \"target\"], inplace=True)\n",
    "    data[\"input\"] = data[\"input\"].astype(str)\n",
    "    data[\"target\"] = data[\"target\"].astype(str)\n",
    "    return list(zip(data[\"input\"], data[\"target\"]))\n",
    "\n",
    "\n",
    "train_pairs = load_data(TRAIN_FILE)\n",
    "dev_pairs = load_data(DEV_FILE)\n",
    "test_pairs = load_data(TEST_FILE)\n",
    "\n",
    "print(\"Sample (roman -> native):\", train_pairs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:46:59.156329Z",
     "iopub.status.busy": "2025-05-20T15:46:59.155805Z",
     "iopub.status.idle": "2025-05-20T15:46:59.309584Z",
     "shell.execute_reply": "2025-05-20T15:46:59.308993Z",
     "shell.execute_reply.started": "2025-05-20T15:46:59.156304Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocab size: 29\n",
      "Target vocab size: 66\n"
     ]
    }
   ],
   "source": [
    "# Special tokens\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "SOS_TOKEN = \"<sos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "\n",
    "def build_vocab(pairs, is_input=True):\n",
    "    counter = Counter()\n",
    "    for src, tgt in pairs:\n",
    "        text = src if is_input else tgt\n",
    "        counter.update(list(text))\n",
    "    chars = sorted(counter.keys())\n",
    "    vocab = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN] + chars\n",
    "    char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "    idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "    return vocab, char2idx, idx2char\n",
    "\n",
    "input_vocab, input2idx, idx2input = build_vocab(train_pairs, is_input=True)\n",
    "target_vocab, target2idx, idx2target = build_vocab(train_pairs, is_input=False)\n",
    "\n",
    "print(f\"Input vocab size: {len(input_vocab)}\")\n",
    "print(f\"Target vocab size: {len(target_vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:47:02.233716Z",
     "iopub.status.busy": "2025-05-20T15:47:02.233451Z",
     "iopub.status.idle": "2025-05-20T15:47:03.416736Z",
     "shell.execute_reply": "2025-05-20T15:47:03.416070Z",
     "shell.execute_reply.started": "2025-05-20T15:47:02.233696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def encode_sequence(seq, char2idx, add_sos_eos=False):\n",
    "    tokens = [char2idx[c] for c in seq]\n",
    "    if add_sos_eos:\n",
    "        return [char2idx[SOS_TOKEN]] + tokens + [char2idx[EOS_TOKEN]]\n",
    "    else:\n",
    "        return tokens\n",
    "\n",
    "def tensorify(pairs, input2idx, target2idx):\n",
    "    input_seqs = [torch.tensor(encode_sequence(src, input2idx), dtype=torch.long) for src, _ in pairs]\n",
    "    target_seqs = [torch.tensor(encode_sequence(tgt, target2idx, add_sos_eos=True), dtype=torch.long) for _, tgt in pairs]\n",
    "    return input_seqs, target_seqs\n",
    "\n",
    "train_input, train_target = tensorify(train_pairs, input2idx, target2idx)\n",
    "dev_input, dev_target = tensorify(dev_pairs, input2idx, target2idx)\n",
    "test_input, test_target = tensorify(test_pairs, input2idx, target2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:47:07.135960Z",
     "iopub.status.busy": "2025-05-20T15:47:07.135371Z",
     "iopub.status.idle": "2025-05-20T15:47:07.250791Z",
     "shell.execute_reply": "2025-05-20T15:47:07.250151Z",
     "shell.execute_reply.started": "2025-05-20T15:47:07.135935Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([64, 17])\n",
      "Target batch shape: torch.Size([64, 14])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, input_seqs, target_seqs):\n",
    "        self.input_seqs = input_seqs\n",
    "        self.target_seqs = target_seqs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_seqs[idx], self.target_seqs[idx]\n",
    "\n",
    "# Collate function to pad a batch\n",
    "def collate_fn(batch):\n",
    "    input_batch, target_batch = zip(*batch)\n",
    "    input_batch = pad_sequence(input_batch, batch_first=True, padding_value=input2idx[PAD_TOKEN])\n",
    "    target_batch = pad_sequence(target_batch, batch_first=True, padding_value=target2idx[PAD_TOKEN])\n",
    "    return input_batch, target_batch\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Seq2SeqDataset(train_input, train_target)\n",
    "dev_dataset = Seq2SeqDataset(dev_input, dev_target)\n",
    "test_dataset = Seq2SeqDataset(test_input, test_target)\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Peek at one batch\n",
    "for x_batch, y_batch in train_loader:\n",
    "    print(\"Input batch shape:\", x_batch.shape)\n",
    "    print(\"Target batch shape:\", y_batch.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:47:13.152695Z",
     "iopub.status.busy": "2025-05-20T15:47:13.152174Z",
     "iopub.status.idle": "2025-05-20T15:47:13.162090Z",
     "shell.execute_reply": "2025-05-20T15:47:13.161532Z",
     "shell.execute_reply.started": "2025-05-20T15:47:13.152672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.2, cell_type=\"LSTM\"):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        rnn_cls = {\"RNN\": nn.RNN, \"GRU\": nn.GRU, \"LSTM\": nn.LSTM}[cell_type]\n",
    "\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.2, cell_type=\"LSTM\"):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        rnn_cls = {\"RNN\": nn.RNN, \"GRU\": nn.GRU, \"LSTM\": nn.LSTM}[cell_type]\n",
    "\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = x.unsqueeze(1)\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        return prediction, hidden\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n",
    "\n",
    "        hidden = self.encoder(src)\n",
    "        input = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:47:19.034077Z",
     "iopub.status.busy": "2025-05-20T15:47:19.033618Z",
     "iopub.status.idle": "2025-05-20T15:47:19.041987Z",
     "shell.execute_reply": "2025-05-20T15:47:19.041308Z",
     "shell.execute_reply.started": "2025-05-20T15:47:19.034054Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_accuracy(preds, targets, pad_idx):\n",
    "    preds = preds.argmax(dim=-1)\n",
    "    mask = targets != pad_idx\n",
    "    correct = (preds == targets) & mask\n",
    "    return correct.sum().item() / mask.sum().item()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, pad_idx, teacher_forcing_ratio):\n",
    "    model.train()\n",
    "    total_loss, total_acc = 0.0, 0.0\n",
    "\n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, tgt, teacher_forcing_ratio)\n",
    "\n",
    "        output = output[:, 1:].reshape(-1, output.size(-1))\n",
    "        tgt = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        acc = compute_accuracy(output, tgt, pad_idx)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, pad_idx):\n",
    "    model.eval()\n",
    "    total_loss, total_acc = 0.0, 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src, tgt, teacher_forcing_ratio=0)\n",
    "\n",
    "            output = output[:, 1:].reshape(-1, output.size(-1))\n",
    "            tgt = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, tgt)\n",
    "            acc = compute_accuracy(output, tgt, pad_idx)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:47:21.863671Z",
     "iopub.status.busy": "2025-05-20T15:47:21.862959Z",
     "iopub.status.idle": "2025-05-20T15:47:21.870477Z",
     "shell.execute_reply": "2025-05-20T15:47:21.869882Z",
     "shell.execute_reply.started": "2025-05-20T15:47:21.863646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_training(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        encoder = Encoder(\n",
    "            vocab_size=len(input_vocab),\n",
    "            embedding_dim=config.embedding_dim,\n",
    "            hidden_dim=config.hidden_dim,\n",
    "            num_layers=config.num_layers,\n",
    "            dropout=config.dropout,\n",
    "            cell_type=config.cell_type\n",
    "        )\n",
    "\n",
    "        decoder = Decoder(\n",
    "            vocab_size=len(target_vocab),\n",
    "            embedding_dim=config.embedding_dim,\n",
    "            hidden_dim=config.hidden_dim,\n",
    "            num_layers=config.num_layers,\n",
    "            dropout=config.dropout,\n",
    "            cell_type=config.cell_type\n",
    "        )\n",
    "\n",
    "        model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=target2idx[PAD_TOKEN])\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device, target2idx[PAD_TOKEN], config.teacher_forcing)\n",
    "            val_loss, val_acc = evaluate(model, dev_loader, criterion, device, target2idx[PAD_TOKEN])\n",
    "\n",
    "            # Log to wandb\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"val_acc\": val_acc\n",
    "            })\n",
    "\n",
    "            # Print to notebook\n",
    "            print(f\"[Epoch {epoch + 1:02}] \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:47:25.662759Z",
     "iopub.status.busy": "2025-05-20T15:47:25.662071Z",
     "iopub.status.idle": "2025-05-20T15:47:25.669934Z",
     "shell.execute_reply": "2025-05-20T15:47:25.669148Z",
     "shell.execute_reply.started": "2025-05-20T15:47:25.662734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_training(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        encoder = Encoder(\n",
    "            vocab_size=len(input_vocab),\n",
    "            embedding_dim=config.embedding_dim,\n",
    "            hidden_dim=config.hidden_dim,\n",
    "            num_layers=config.num_layers,\n",
    "            dropout=config.dropout,\n",
    "            cell_type=config.cell_type\n",
    "        )\n",
    "\n",
    "        decoder = Decoder(\n",
    "            vocab_size=len(target_vocab),\n",
    "            embedding_dim=config.embedding_dim,\n",
    "            hidden_dim=config.hidden_dim,\n",
    "            num_layers=config.num_layers,\n",
    "            dropout=config.dropout,\n",
    "            cell_type=config.cell_type\n",
    "        )\n",
    "\n",
    "        model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=target2idx[PAD_TOKEN])\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            train_loss, train_acc = train_one_epoch(\n",
    "                model, train_loader, optimizer, criterion,\n",
    "                device, target2idx[PAD_TOKEN], config.teacher_forcing\n",
    "            )\n",
    "\n",
    "            val_loss, val_acc = evaluate(\n",
    "                model, dev_loader, criterion,\n",
    "                device, target2idx[PAD_TOKEN]\n",
    "            )\n",
    "\n",
    "            is_best = val_acc > best_val_acc\n",
    "            if is_best:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "            # Log to WandB\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"val_acc\": val_acc\n",
    "            })\n",
    "\n",
    "            # Print to notebook\n",
    "            print(f\"[Epoch {epoch + 1:02}] \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} \"\n",
    "                  f\"{'(Saved)' if is_best else ''}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T10:11:25.025322Z",
     "iopub.status.busy": "2025-05-18T10:11:25.024881Z",
     "iopub.status.idle": "2025-05-18T10:11:25.029504Z",
     "shell.execute_reply": "2025-05-18T10:11:25.028932Z",
     "shell.execute_reply.started": "2025-05-18T10:11:25.025301Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# wandb.init(project=\"DA6401_Assignment_3\", name=\"baseline-run_1\")\n",
    "\n",
    "# config = {\n",
    "#     \"embedding_dim\": 64,\n",
    "#     \"hidden_dim\": 128,\n",
    "#     \"enc_layers\": 1,\n",
    "#     \"dec_layers\": 1,\n",
    "#     \"dropout\": 0.3,\n",
    "#     \"cell_type\": \"GRU\",\n",
    "#     \"lr\": 0.001,\n",
    "#     \"epochs\": 20,\n",
    "#     \"teacher_forcing\": 0.5,\n",
    "# }\n",
    "#run_training(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T14:22:27.311441Z",
     "iopub.status.busy": "2025-05-19T14:22:27.310880Z",
     "iopub.status.idle": "2025-05-19T14:22:27.315867Z",
     "shell.execute_reply": "2025-05-19T14:22:27.315212Z",
     "shell.execute_reply.started": "2025-05-19T14:22:27.311419Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"bayes\",  # could also be \"grid\" or \"bayes\"\n",
    "    \"metric\": {\n",
    "        \"name\": \"val_acc\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"embedding_dim\": {\"values\": [32, 64, 128, 256]},\n",
    "        \"hidden_dim\": {\"values\": [32, 64, 128, 256]},\n",
    "        \"num_layers\": {\"values\": [1, 2, 3]},\n",
    "        \"dropout\": {\"values\": [0.0, 0.2, 0.4, 0.5]},\n",
    "        \"cell_type\": {\"values\": [\"RNN\", \"GRU\", \"LSTM\"]},\n",
    "        \"lr\": {\"values\": [0.001, 0.0005, 0.0001]},\n",
    "        \"teacher_forcing\": {\"values\": [0.5, 0.7]},\n",
    "        \"epochs\": {\"value\": 15}\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T14:22:27.570668Z",
     "iopub.status.busy": "2025-05-19T14:22:27.570409Z",
     "iopub.status.idle": "2025-05-19T14:22:27.573938Z",
     "shell.execute_reply": "2025-05-19T14:22:27.573399Z",
     "shell.execute_reply.started": "2025-05-19T14:22:27.570651Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# sweep_id = wandb.sweep(sweep_config, project=\"DA6401_Assignment_3\")\n",
    "# wandb.agent(sweep_id, function=run_training, count=25)  # You can increase count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T15:38:20.354375Z",
     "iopub.status.busy": "2025-05-19T15:38:20.354080Z",
     "iopub.status.idle": "2025-05-19T15:38:20.358358Z",
     "shell.execute_reply": "2025-05-19T15:38:20.357653Z",
     "shell.execute_reply.started": "2025-05-19T15:38:20.354355Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_config = {\n",
    "    \"embedding_dim\": 64,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_layers\": 3,\n",
    "    \"dropout\": 0.5,\n",
    "    \"cell_type\": \"LSTM\",\n",
    "    \"lr\": 0.001,\n",
    "    \"teacher_forcing\": 0.5,\n",
    "    \"epochs\": 15\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T15:38:22.104002Z",
     "iopub.status.busy": "2025-05-19T15:38:22.103746Z",
     "iopub.status.idle": "2025-05-19T15:47:21.088080Z",
     "shell.execute_reply": "2025-05-19T15:47:21.087400Z",
     "shell.execute_reply.started": "2025-05-19T15:38:22.103983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# wandb.init(project=\"DA6401_Assignment_3\", name=\"best_config_rerun\")\n",
    "# run_training(best_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T17:23:23.274157Z",
     "iopub.status.busy": "2025-05-19T17:23:23.273902Z",
     "iopub.status.idle": "2025-05-19T17:23:23.331581Z",
     "shell.execute_reply": "2025-05-19T17:23:23.330686Z",
     "shell.execute_reply.started": "2025-05-19T17:23:23.274139Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Use the same best_config used during training\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# encoder = Encoder(\n",
    "#     vocab_size=len(input_vocab),\n",
    "#     embedding_dim=best_config[\"embedding_dim\"],\n",
    "#     hidden_dim=best_config[\"hidden_dim\"],\n",
    "#     num_layers=best_config[\"num_layers\"],\n",
    "#     dropout=best_config[\"dropout\"],\n",
    "#     cell_type=best_config[\"cell_type\"]\n",
    "# )\n",
    "\n",
    "# decoder = Decoder(\n",
    "#     vocab_size=len(target_vocab),\n",
    "#     embedding_dim=best_config[\"embedding_dim\"],\n",
    "#     hidden_dim=best_config[\"hidden_dim\"],\n",
    "#     num_layers=best_config[\"num_layers\"],\n",
    "#     dropout=best_config[\"dropout\"],\n",
    "#     cell_type=best_config[\"cell_type\"]\n",
    "# )\n",
    "\n",
    "# model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "# model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T17:23:25.547313Z",
     "iopub.status.busy": "2025-05-19T17:23:25.547020Z",
     "iopub.status.idle": "2025-05-19T17:23:25.568756Z",
     "shell.execute_reply": "2025-05-19T17:23:25.565575Z",
     "shell.execute_reply.started": "2025-05-19T17:23:25.547289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# sample_input = pad_sequence(test_input[:1], batch_first=True, padding_value=input2idx[PAD_TOKEN]).to(device)\n",
    "# sample_target = pad_sequence(test_target[:1], batch_first=True, padding_value=target2idx[PAD_TOKEN]).to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output = model(sample_input, sample_target, teacher_forcing_ratio=0.0)\n",
    "#     print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:47:42.923096Z",
     "iopub.status.busy": "2025-05-20T15:47:42.922438Z",
     "iopub.status.idle": "2025-05-20T15:47:43.009233Z",
     "shell.execute_reply": "2025-05-20T15:47:43.008497Z",
     "shell.execute_reply.started": "2025-05-20T15:47:42.923071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tensorify(pairs, input2idx, target2idx, add_sos_eos=True):\n",
    "    input_seqs = [torch.tensor(encode_sequence(src, input2idx), dtype=torch.long) for src, _ in pairs]\n",
    "    target_seqs = [torch.tensor(encode_sequence(tgt, target2idx, add_sos_eos=add_sos_eos), dtype=torch.long) for _, tgt in pairs]\n",
    "    return input_seqs, target_seqs\n",
    "test_input, test_target = tensorify(test_pairs, input2idx, target2idx, add_sos_eos=True)  # ‚úÖ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:47:47.235986Z",
     "iopub.status.busy": "2025-05-20T15:47:47.235235Z",
     "iopub.status.idle": "2025-05-20T15:47:47.241783Z",
     "shell.execute_reply": "2025-05-20T15:47:47.241147Z",
     "shell.execute_reply.started": "2025-05-20T15:47:47.235961Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_exact_match(model, dataloader, device, idx2target, pad_idx):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    predictions = []\n",
    "\n",
    "    ignore = {pad_idx, target2idx[SOS_TOKEN], target2idx[EOS_TOKEN]}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src, tgt, teacher_forcing_ratio=0.0)\n",
    "            preds = output.argmax(dim=-1)\n",
    "\n",
    "            for i in range(src.size(0)):\n",
    "                tgt_str = \"\".join([idx2target[idx.item()] for idx in tgt[i] if idx.item() not in ignore])\n",
    "                pred_str = \"\".join([idx2target[idx.item()] for idx in preds[i] if idx.item() not in ignore])\n",
    "                predictions.append((tgt_str, pred_str))\n",
    "                if pred_str == tgt_str:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T16:08:15.042932Z",
     "iopub.status.busy": "2025-05-19T16:08:15.042454Z",
     "iopub.status.idle": "2025-05-19T16:08:16.052155Z",
     "shell.execute_reply": "2025-05-19T16:08:16.051511Z",
     "shell.execute_reply.started": "2025-05-19T16:08:15.042911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test_acc, test_preds = evaluate_exact_match(model, test_loader, device, idx2target, target2idx[PAD_TOKEN])\n",
    "# print(f\"\\n‚úÖ Exact Match Accuracy on Test Set: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T16:08:48.553362Z",
     "iopub.status.busy": "2025-05-19T16:08:48.552840Z",
     "iopub.status.idle": "2025-05-19T16:08:48.576538Z",
     "shell.execute_reply": "2025-05-19T16:08:48.575854Z",
     "shell.execute_reply.started": "2025-05-19T16:08:48.553343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Get Roman inputs again\n",
    "# roman_inputs = [x[0] for x in test_pairs]\n",
    "\n",
    "# # Sanity check: match input length\n",
    "# assert len(roman_inputs) == len(test_preds)\n",
    "\n",
    "# # Create DataFrame\n",
    "# df_preds = pd.DataFrame({\n",
    "#     \"Input (Roman)\": roman_inputs,\n",
    "#     \"Ground Truth (Telugu)\": [gt for gt, _ in test_preds],\n",
    "#     \"Prediction (Telugu)\": [pred for _, pred in test_preds],\n",
    "#     \"Match\": [gt == pred for gt, pred in test_preds]\n",
    "# })\n",
    "\n",
    "# # Save to predictions_vanilla folder\n",
    "# Path(\"predictions_vanilla\").mkdir(exist_ok=True)\n",
    "# df_preds.to_csv(\"predictions_vanilla/test_predictions_old.csv\", index=False)\n",
    "\n",
    "# # Preview top rows\n",
    "# df_preds.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T16:08:52.117814Z",
     "iopub.status.busy": "2025-05-19T16:08:52.117043Z",
     "iopub.status.idle": "2025-05-19T16:08:52.130577Z",
     "shell.execute_reply": "2025-05-19T16:08:52.130026Z",
     "shell.execute_reply.started": "2025-05-19T16:08:52.117792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from IPython.display import display, HTML\n",
    "\n",
    "# # Sample 8 examples (some matches, some mismatches)\n",
    "# sampled_df = df_preds.sample(n=8, random_state=42)\n",
    "\n",
    "# # Add green check / red cross to Match column\n",
    "# def match_icon(match):\n",
    "#     return \"‚úÖ\" if match else \"‚ùå\"\n",
    "\n",
    "# sampled_df[\"Correct\"] = sampled_df[\"Match\"].apply(match_icon)\n",
    "# sampled_df = sampled_df.drop(columns=[\"Match\"])\n",
    "\n",
    "# # Style and display as HTML table\n",
    "# display(HTML(sampled_df.to_html(index=False)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:47:54.843143Z",
     "iopub.status.busy": "2025-05-20T15:47:54.842595Z",
     "iopub.status.idle": "2025-05-20T15:47:54.848170Z",
     "shell.execute_reply": "2025-05-20T15:47:54.847490Z",
     "shell.execute_reply.started": "2025-05-20T15:47:54.843124Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AttentionEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout, cell_type=\"LSTM\"):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        rnn_cls = {\"RNN\": nn.RNN, \"GRU\": nn.GRU, \"LSTM\": nn.LSTM}[cell_type]\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)  # [batch, src_len, emb_dim]\n",
    "        outputs, hidden = self.rnn(embedded)  # outputs: [batch, src_len, hidden_dim]\n",
    "        return outputs, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:47:55.133786Z",
     "iopub.status.busy": "2025-05-20T15:47:55.133589Z",
     "iopub.status.idle": "2025-05-20T15:47:55.138671Z",
     "shell.execute_reply": "2025-05-20T15:47:55.138074Z",
     "shell.execute_reply.started": "2025-05-20T15:47:55.133772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(encoder_hidden_dim + decoder_hidden_dim, decoder_hidden_dim)\n",
    "        self.v = nn.Linear(decoder_hidden_dim, 1, bias=False)  # üîÑ Replaces raw vector\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [batch, dec_hidden_dim]\n",
    "        # encoder_outputs: [batch, src_len, enc_hidden_dim]\n",
    "\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        src_len = encoder_outputs.size(1)\n",
    "\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch, src_len, dec_hidden_dim]\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch, src_len, dec_hidden_dim]\n",
    "\n",
    "        # üîÅ v is now a linear layer\n",
    "        attention = self.v(energy).squeeze(2)  # [batch, src_len]\n",
    "\n",
    "        return F.softmax(attention, dim=1)  # [batch, src_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:47:57.507896Z",
     "iopub.status.busy": "2025-05-20T15:47:57.507132Z",
     "iopub.status.idle": "2025-05-20T15:47:57.515426Z",
     "shell.execute_reply": "2025-05-20T15:47:57.514692Z",
     "shell.execute_reply.started": "2025-05-20T15:47:57.507869Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_hidden_dim, dec_hidden_dim, dropout, cell_type=\"LSTM\"):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.attention = BahdanauAttention(enc_hidden_dim, dec_hidden_dim)\n",
    "        self.rnn_input_dim = embedding_dim + enc_hidden_dim\n",
    "\n",
    "        rnn_cls = {\"RNN\": nn.RNN, \"GRU\": nn.GRU, \"LSTM\": nn.LSTM}[cell_type]\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=self.rnn_input_dim,\n",
    "            hidden_size=dec_hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(enc_hidden_dim + dec_hidden_dim + embedding_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "    def forward(self, input_token, hidden, encoder_outputs):\n",
    "        # input_token: [batch]\n",
    "        # hidden: [1, batch, dec_hidden_dim] or (h, c)\n",
    "        # encoder_outputs: [batch, src_len, enc_hidden_dim]\n",
    "\n",
    "        input_token = input_token.unsqueeze(1)  # [batch, 1]\n",
    "        embedded = self.dropout(self.embedding(input_token))  # [batch, 1, emb_dim]\n",
    "\n",
    "        if self.cell_type == \"LSTM\":\n",
    "            decoder_hidden = hidden[0][-1]  # last layer's hidden state\n",
    "        else:\n",
    "            decoder_hidden = hidden[-1]  # [batch, dec_hidden_dim]\n",
    "\n",
    "        attn_weights = self.attention(decoder_hidden, encoder_outputs)  # [batch, src_len]\n",
    "        attn_weights = attn_weights.unsqueeze(1)  # [batch, 1, src_len]\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)  # [batch, 1, enc_hidden_dim]\n",
    "\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)  # [batch, 1, emb+context]\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "\n",
    "        output = output.squeeze(1)      # [batch, dec_hidden_dim]\n",
    "        context = context.squeeze(1)    # [batch, enc_hidden_dim]\n",
    "        embedded = embedded.squeeze(1)  # [batch, emb_dim]\n",
    "\n",
    "        output = self.fc_out(torch.cat((output, context, embedded), dim=1))  # [batch, vocab_size]\n",
    "        return output, hidden, attn_weights.squeeze(1)  # return attention weights for visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:48:00.614343Z",
     "iopub.status.busy": "2025-05-20T15:48:00.613911Z",
     "iopub.status.idle": "2025-05-20T15:48:00.620653Z",
     "shell.execute_reply": "2025-05-20T15:48:00.619865Z",
     "shell.execute_reply.started": "2025-05-20T15:48:00.614322Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AttentionSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n",
    "        attentions = torch.zeros(batch_size, trg_len, src.size(1)).to(self.device)\n",
    "\n",
    "        # Encoder forward\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        # Initialize first input to decoder\n",
    "        input_token = trg[:, 0]  # <sos>\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, attn_weights = self.decoder(input_token, hidden, encoder_outputs)\n",
    "            outputs[:, t] = output\n",
    "            attentions[:, t] = attn_weights\n",
    "\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input_token = trg[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs, attentions  # [batch, trg_len, vocab_size], [batch, trg_len, src_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:48:01.271239Z",
     "iopub.status.busy": "2025-05-20T15:48:01.270445Z",
     "iopub.status.idle": "2025-05-20T15:48:01.279554Z",
     "shell.execute_reply": "2025-05-20T15:48:01.278986Z",
     "shell.execute_reply.started": "2025-05-20T15:48:01.271205Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_attention_training():\n",
    "    with wandb.init() as run:\n",
    "        config = run.config\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Encoder and Decoder setup\n",
    "        encoder = AttentionEncoder(\n",
    "            vocab_size=len(input_vocab),\n",
    "            embedding_dim=config.embedding_dim,\n",
    "            hidden_dim=config.hidden_dim,\n",
    "            dropout=config.dropout,\n",
    "            cell_type=config.cell_type\n",
    "        )\n",
    "\n",
    "        decoder = AttentionDecoder(\n",
    "            vocab_size=len(target_vocab),\n",
    "            embedding_dim=config.embedding_dim,\n",
    "            enc_hidden_dim=config.hidden_dim,\n",
    "            dec_hidden_dim=config.hidden_dim,\n",
    "            dropout=config.dropout,\n",
    "            cell_type=config.cell_type\n",
    "        )\n",
    "\n",
    "        model = AttentionSeq2Seq(encoder, decoder, device).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=target2idx[PAD_TOKEN])\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            model.train()\n",
    "            total_loss, total_acc = 0.0, 0.0\n",
    "\n",
    "            for src, tgt in train_loader:\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output, _ = model(src, tgt, teacher_forcing_ratio=config.teacher_forcing)\n",
    "\n",
    "                output = output[:, 1:].reshape(-1, output.size(-1))\n",
    "                tgt_gold = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "                loss = criterion(output, tgt_gold)\n",
    "                acc = compute_accuracy(output, tgt_gold, target2idx[PAD_TOKEN])\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_acc += acc\n",
    "\n",
    "            train_loss = total_loss / len(train_loader)\n",
    "            train_acc = total_acc / len(train_loader)\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_acc = evaluate_attention_model(model, dev_loader, criterion, device, target2idx[PAD_TOKEN])\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(model.state_dict(), \"best_model_attn.pth\")\n",
    "\n",
    "            # Logging\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"val_acc\": val_acc\n",
    "            })\n",
    "\n",
    "            print(f\"[Epoch {epoch + 1:02}] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} \"\n",
    "                  f\"| Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} \"\n",
    "                  f\"{'(Saved)' if val_acc == best_val_acc else ''}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:48:05.980721Z",
     "iopub.status.busy": "2025-05-20T15:48:05.980461Z",
     "iopub.status.idle": "2025-05-20T15:48:05.986647Z",
     "shell.execute_reply": "2025-05-20T15:48:05.986038Z",
     "shell.execute_reply.started": "2025-05-20T15:48:05.980701Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_attention_model(model, dataloader, criterion, device, pad_idx):\n",
    "    model.eval()\n",
    "    total_loss, total_acc = 0.0, 0.0\n",
    "    attn_sum = 0.0\n",
    "    attn_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output, attentions = model(src, tgt, teacher_forcing_ratio=0.0)\n",
    "\n",
    "            output = output[:, 1:].reshape(-1, output.size(-1))\n",
    "            tgt_gold = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, tgt_gold)\n",
    "            acc = compute_accuracy(output, tgt_gold, pad_idx)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "\n",
    "            # üîç Track mean attention value\n",
    "            attn_sum += attentions.sum().item()\n",
    "            attn_count += attentions.numel()\n",
    "\n",
    "    mean_attn = attn_sum / attn_count if attn_count > 0 else 0.0\n",
    "    print(f\"üîç Mean Attention Weight: {mean_attn:.4f}\")\n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T16:41:58.721995Z",
     "iopub.status.busy": "2025-05-19T16:41:58.721732Z",
     "iopub.status.idle": "2025-05-19T16:49:10.810964Z",
     "shell.execute_reply": "2025-05-19T16:49:10.810466Z",
     "shell.execute_reply.started": "2025-05-19T16:41:58.721976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_attn_config = {\n",
    "    \"embedding_dim\": 128,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"dropout\": 0.5,\n",
    "    \"cell_type\": \"LSTM\",\n",
    "    \"lr\": 0.001,\n",
    "    \"teacher_forcing\": 0.5,\n",
    "    \"epochs\": 10\n",
    "}\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Run manually without sweep\n",
    "wandb.init(project=\"DA6401_Assignment_3\", name=\"attention_single_run\", config=best_attn_config)\n",
    "run_attention_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T17:19:40.727039Z",
     "iopub.status.busy": "2025-05-19T17:19:40.726415Z",
     "iopub.status.idle": "2025-05-19T17:21:01.253063Z",
     "shell.execute_reply": "2025-05-19T17:21:01.252178Z",
     "shell.execute_reply.started": "2025-05-19T17:19:40.727010Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"val_acc\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"embedding_dim\": {\"values\": [32, 64, 128, 256]},\n",
    "        \"hidden_dim\": {\"values\": [32, 64, 128, 256]},\n",
    "        \"dropout\": {\"values\": [0.0, 0.2, 0.4, 0.5]},\n",
    "        \"cell_type\": {\"values\": [\"RNN\", \"GRU\", \"LSTM\"]},\n",
    "        \"lr\": {\"values\": [0.001, 0.0005, 0.0001]},\n",
    "        \"teacher_forcing\": {\"values\": [0.5, 0.7]},\n",
    "        \"epochs\": {\"value\": 10}\n",
    "    }\n",
    "}\n",
    "\n",
    "# 1. Create the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"DA6401_Assignment_3\")\n",
    "\n",
    "# 2. Run the sweep with 25 trials (you can increase if needed)\n",
    "wandb.agent(sweep_id, function=run_attention_training, count=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T14:22:52.309417Z",
     "iopub.status.busy": "2025-05-19T14:22:52.308855Z",
     "iopub.status.idle": "2025-05-19T14:22:52.313563Z",
     "shell.execute_reply": "2025-05-19T14:22:52.312900Z",
     "shell.execute_reply.started": "2025-05-19T14:22:52.309395Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# sweep_config = {\n",
    "#     \"method\": \"bayes\",  # could also be \"grid\" or \"bayes\"\n",
    "#     \"metric\": {\n",
    "#         \"name\": \"val_acc\",\n",
    "#         \"goal\": \"maximize\"\n",
    "#     },\n",
    "#     \"parameters\": {\n",
    "#         \"embedding_dim\": {\"values\": [32, 64, 128, 256]},\n",
    "#         \"hidden_dim\": {\"values\": [32, 64, 128, 256]},\n",
    "#         \"dropout\": {\"values\": [0.0, 0.2, 0.4, 0.5]},\n",
    "#         \"cell_type\": {\"values\": [\"RNN\", \"GRU\", \"LSTM\"]},\n",
    "#         \"lr\": {\"values\": [0.001, 0.0005, 0.0001]},\n",
    "#         \"teacher_forcing\": {\"values\": [0.5, 0.7]},\n",
    "#         \"epochs\": {\"value\": 10}\n",
    "#     }\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T15:35:28.186843Z",
     "iopub.status.busy": "2025-05-18T15:35:28.186564Z",
     "iopub.status.idle": "2025-05-18T15:45:49.479332Z",
     "shell.execute_reply": "2025-05-18T15:45:49.478817Z",
     "shell.execute_reply.started": "2025-05-18T15:35:28.186823Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# # Create sweep\n",
    "# sweep_id = wandb.sweep(sweep_config, project=\"DA6401_Assignment_3\")\n",
    "\n",
    "# # Run the sweep with N trials\n",
    "# wandb.agent(sweep_id, function=run_attention_training, count=25)  # Increase count if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:48:18.688918Z",
     "iopub.status.busy": "2025-05-20T15:48:18.688660Z",
     "iopub.status.idle": "2025-05-20T15:48:18.698912Z",
     "shell.execute_reply": "2025-05-20T15:48:18.698141Z",
     "shell.execute_reply.started": "2025-05-20T15:48:18.688898Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_attention_training():\n",
    "    best_attn_config = {\n",
    "        \"embedding_dim\": 256,\n",
    "        \"hidden_dim\": 256,\n",
    "        \"dropout\": 0.2,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"lr\": 0.001,\n",
    "        \"teacher_forcing\": 0.7,\n",
    "        \"epochs\": 10\n",
    "    }\n",
    "\n",
    "    with wandb.init(project=\"DA6401_Assignment_3\", name=\"best_attention_final\", config=best_attn_config):\n",
    "        config = wandb.config\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        encoder = AttentionEncoder(\n",
    "            vocab_size=len(input_vocab),\n",
    "            embedding_dim=config.embedding_dim,\n",
    "            hidden_dim=config.hidden_dim,\n",
    "            dropout=config.dropout,\n",
    "            cell_type=config.cell_type\n",
    "        )\n",
    "\n",
    "        decoder = AttentionDecoder(\n",
    "            vocab_size=len(target_vocab),\n",
    "            embedding_dim=config.embedding_dim,\n",
    "            enc_hidden_dim=config.hidden_dim,\n",
    "            dec_hidden_dim=config.hidden_dim,\n",
    "            dropout=config.dropout,\n",
    "            cell_type=config.cell_type\n",
    "        )\n",
    "\n",
    "        model = AttentionSeq2Seq(encoder, decoder, device).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=target2idx[PAD_TOKEN])\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            model.train()\n",
    "            total_loss, total_acc = 0.0, 0.0\n",
    "\n",
    "            for src, tgt in train_loader:\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output, _ = model(src, tgt, teacher_forcing_ratio=config.teacher_forcing)\n",
    "\n",
    "                output = output[:, 1:].reshape(-1, output.size(-1))\n",
    "                tgt_gold = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "                loss = criterion(output, tgt_gold)\n",
    "                acc = compute_accuracy(output, tgt_gold, target2idx[PAD_TOKEN])\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_acc += acc\n",
    "\n",
    "            train_loss = total_loss / len(train_loader)\n",
    "            train_acc = total_acc / len(train_loader)\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_acc = evaluate_attention_model(model, dev_loader, criterion, device, target2idx[PAD_TOKEN])\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(model.state_dict(), \"best_model_attn.pth\")\n",
    "\n",
    "            # Logging\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"val_acc\": val_acc\n",
    "            })\n",
    "\n",
    "            print(f\"[Epoch {epoch + 1:02}] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} \"\n",
    "                  f\"| Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} \"\n",
    "                  f\"{'(Saved)' if val_acc == best_val_acc else ''}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T13:18:57.058452Z",
     "iopub.status.busy": "2025-05-20T13:18:57.058237Z",
     "iopub.status.idle": "2025-05-20T13:25:28.063041Z",
     "shell.execute_reply": "2025-05-20T13:25:28.062383Z",
     "shell.execute_reply.started": "2025-05-20T13:18:57.058437Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# run_attention_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:51:40.928954Z",
     "iopub.status.busy": "2025-05-20T15:51:40.928139Z",
     "iopub.status.idle": "2025-05-20T15:51:41.396388Z",
     "shell.execute_reply": "2025-05-20T15:51:41.395823Z",
     "shell.execute_reply.started": "2025-05-20T15:51:40.928918Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded best attention model successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sathwikpentela/miniforge3/envs/PINN/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# === Configuration used during training ===\n",
    "best_attn_config = {\n",
    "    \"embedding_dim\": 256,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"dropout\": 0.2,\n",
    "    \"cell_type\": \"LSTM\"\n",
    "}\n",
    "\n",
    "# === Set device ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Rebuild the model architecture ===\n",
    "encoder = AttentionEncoder(\n",
    "    vocab_size=len(input_vocab),\n",
    "    embedding_dim=best_attn_config[\"embedding_dim\"],\n",
    "    hidden_dim=best_attn_config[\"hidden_dim\"],\n",
    "    dropout=best_attn_config[\"dropout\"],\n",
    "    cell_type=best_attn_config[\"cell_type\"]\n",
    ")\n",
    "\n",
    "decoder = AttentionDecoder(\n",
    "    vocab_size=len(target_vocab),\n",
    "    embedding_dim=best_attn_config[\"embedding_dim\"],\n",
    "    enc_hidden_dim=best_attn_config[\"hidden_dim\"],\n",
    "    dec_hidden_dim=best_attn_config[\"hidden_dim\"],\n",
    "    dropout=best_attn_config[\"dropout\"],\n",
    "    cell_type=best_attn_config[\"cell_type\"]\n",
    ")\n",
    "\n",
    "model_attn = AttentionSeq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# === Load weights from saved .pth file ===\n",
    "checkpoint_path = \"best_model_attn (2).pth\"\n",
    "model_attn.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model_attn.eval()\n",
    "\n",
    "print(\"‚úÖ Loaded best attention model successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:51:52.347416Z",
     "iopub.status.busy": "2025-05-20T15:51:52.346914Z",
     "iopub.status.idle": "2025-05-20T15:51:52.351192Z",
     "shell.execute_reply": "2025-05-20T15:51:52.350494Z",
     "shell.execute_reply.started": "2025-05-20T15:51:52.347395Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def detokenize(token_ids, idx2char):\n",
    "    \"\"\"Converts list of token ids to string (removes special tokens like PAD/SOS/EOS).\"\"\"\n",
    "    return ''.join([idx2char[idx] for idx in token_ids if idx2char[idx] not in [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:51:55.615844Z",
     "iopub.status.busy": "2025-05-20T15:51:55.615607Z",
     "iopub.status.idle": "2025-05-20T15:51:56.328509Z",
     "shell.execute_reply": "2025-05-20T15:51:56.327938Z",
     "shell.execute_reply.started": "2025-05-20T15:51:55.615828Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:52:03.353752Z",
     "iopub.status.busy": "2025-05-20T15:52:03.353187Z",
     "iopub.status.idle": "2025-05-20T15:52:03.358460Z",
     "shell.execute_reply": "2025-05-20T15:52:03.357630Z",
     "shell.execute_reply.started": "2025-05-20T15:52:03.353723Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:52:03.580164Z",
     "iopub.status.busy": "2025-05-20T15:52:03.579439Z",
     "iopub.status.idle": "2025-05-20T15:52:03.591599Z",
     "shell.execute_reply": "2025-05-20T15:52:03.588127Z",
     "shell.execute_reply.started": "2025-05-20T15:52:03.580140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text, vocab, add_sos_eos=False):\n",
    "    tokens = [vocab[char] for char in text if char in vocab]\n",
    "    if add_sos_eos:\n",
    "        tokens = [vocab[SOS_TOKEN]] + tokens + [vocab[EOS_TOKEN]]\n",
    "    return tokens\n",
    "\n",
    "def detokenize(token_ids, idx2char):\n",
    "    return ''.join([idx2char[idx] for idx in token_ids if idx in idx2char and idx2char[idx] not in [SOS_TOKEN, EOS_TOKEN, PAD_TOKEN]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:51:52.607740Z",
     "iopub.status.busy": "2025-05-20T15:51:52.607512Z",
     "iopub.status.idle": "2025-05-20T15:51:52.616464Z",
     "shell.execute_reply": "2025-05-20T15:51:52.615755Z",
     "shell.execute_reply.started": "2025-05-20T15:51:52.607722Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "# Load the Telugu-compatible font\n",
    "telugu_font = FontProperties(fname=\"Noto_Sans_Telugu/static/NotoSansTelugu-Black.ttf\")\n",
    "\n",
    "def visualize_attention(model, input_text, input2idx, idx2target, target2idx, device, max_len=30):\n",
    "    model.eval()\n",
    "    src_tensor = torch.tensor([tokenize(input_text, input2idx, add_sos_eos=True)], device=device)\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "\n",
    "    input_chars = [SOS_TOKEN] + list(input_text) + [EOS_TOKEN]\n",
    "    pred_chars = []\n",
    "    attentions = []\n",
    "\n",
    "    input_token = torch.tensor([target2idx[SOS_TOKEN]], device=device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output, hidden, attn_weights = model.decoder(input_token, hidden, encoder_outputs)\n",
    "            top1 = output.argmax(1).item()\n",
    "            pred_chars.append(idx2target[top1])\n",
    "            attentions.append(attn_weights.squeeze(0).cpu().numpy())\n",
    "        if idx2target[top1] == EOS_TOKEN:\n",
    "            break\n",
    "        input_token = torch.tensor([top1], device=device)\n",
    "\n",
    "    attn_matrix = np.stack(attentions)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(attn_matrix, xticklabels=input_chars, yticklabels=pred_chars, cmap=\"viridis\")\n",
    "\n",
    "    plt.xticks(fontproperties=telugu_font, rotation=0, fontsize=12)\n",
    "    plt.yticks(fontproperties=telugu_font, rotation=0, fontsize=12)\n",
    "    plt.xlabel(\"Input Characters\", fontproperties=telugu_font)\n",
    "    plt.ylabel(\"Predicted Characters\", fontproperties=telugu_font)\n",
    "    plt.title(f\"Attention Heatmap for: {input_text}\", fontproperties=telugu_font, fontsize=14)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:52:24.845355Z",
     "iopub.status.busy": "2025-05-20T15:52:24.845079Z",
     "iopub.status.idle": "2025-05-20T15:52:25.049923Z",
     "shell.execute_reply": "2025-05-20T15:52:25.049103Z",
     "shell.execute_reply.started": "2025-05-20T15:52:24.845331Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sathwikpentela/miniforge3/envs/PINN/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning: Glyph 3112 (\\N{TELUGU LETTER NA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "/Users/sathwikpentela/miniforge3/envs/PINN/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning: Matplotlib currently does not support Telugu natively.\n",
      "  fig.canvas.draw()\n",
      "/Users/sathwikpentela/miniforge3/envs/PINN/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning: Glyph 3143 (\\N{TELUGU VOWEL SIGN EE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "/Users/sathwikpentela/miniforge3/envs/PINN/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning: Glyph 3107 (\\N{TELUGU LETTER NNA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "/Users/sathwikpentela/miniforge3/envs/PINN/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning: Glyph 3137 (\\N{TELUGU VOWEL SIGN U}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "/Users/sathwikpentela/miniforge3/envs/PINN/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning: Glyph 3138 (\\N{TELUGU VOWEL SIGN UU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_attention(model_attn, \"nenu\", input2idx, idx2target, target2idx, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:52:20.329818Z",
     "iopub.status.busy": "2025-05-20T15:52:20.329078Z",
     "iopub.status.idle": "2025-05-20T15:52:20.363919Z",
     "shell.execute_reply": "2025-05-20T15:52:20.363172Z",
     "shell.execute_reply.started": "2025-05-20T15:52:20.329795Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sathwikpentela/miniforge3/envs/PINN/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AttentionSeq2Seq(\n",
       "  (encoder): AttentionEncoder(\n",
       "    (embedding): Embedding(29, 256)\n",
       "    (rnn): LSTM(256, 256, batch_first=True, dropout=0.2)\n",
       "  )\n",
       "  (decoder): AttentionDecoder(\n",
       "    (embedding): Embedding(66, 256)\n",
       "    (attention): BahdanauAttention(\n",
       "      (attn): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (v): Linear(in_features=256, out_features=1, bias=False)\n",
       "    )\n",
       "    (rnn): LSTM(512, 256, batch_first=True)\n",
       "    (fc_out): Linear(in_features=768, out_features=66, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreate the model with best config\n",
    "encoder = AttentionEncoder(\n",
    "    vocab_size=len(input_vocab),\n",
    "    embedding_dim=256,\n",
    "    hidden_dim=256,\n",
    "    dropout=0.2,\n",
    "    cell_type=\"LSTM\"\n",
    ")\n",
    "\n",
    "decoder = AttentionDecoder(\n",
    "    vocab_size=len(target_vocab),\n",
    "    embedding_dim=256,\n",
    "    enc_hidden_dim=256,\n",
    "    dec_hidden_dim=256,\n",
    "    dropout=0.2,\n",
    "    cell_type=\"LSTM\"\n",
    ")\n",
    "\n",
    "model_attn = AttentionSeq2Seq(encoder, decoder, device).to(device)\n",
    "model_attn.load_state_dict(torch.load(\"best_model_attn (2).pth\", map_location=torch.device('cpu')))\n",
    "model_attn.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:52:32.403991Z",
     "iopub.status.busy": "2025-05-20T15:52:32.403390Z",
     "iopub.status.idle": "2025-05-20T15:52:32.409878Z",
     "shell.execute_reply": "2025-05-20T15:52:32.409108Z",
     "shell.execute_reply.started": "2025-05-20T15:52:32.403969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_exact_match(model, dataloader, device, idx2target, pad_idx):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    predictions = []\n",
    "    eos_idx = target2idx[EOS_TOKEN]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output, _ = model(src, tgt, teacher_forcing_ratio=0.0)\n",
    "\n",
    "            preds = output.argmax(dim=-1)\n",
    "\n",
    "            for i in range(src.size(0)):\n",
    "                pred_tokens = preds[i].tolist()\n",
    "                tgt_tokens = tgt[i].tolist()\n",
    "\n",
    "                if eos_idx in pred_tokens:\n",
    "                    pred_tokens = pred_tokens[:pred_tokens.index(eos_idx)]\n",
    "                if eos_idx in tgt_tokens:\n",
    "                    tgt_tokens = tgt_tokens[:tgt_tokens.index(eos_idx)]\n",
    "\n",
    "                pred_str = detokenize(pred_tokens, idx2target)\n",
    "                tgt_str = detokenize(tgt_tokens, idx2target)\n",
    "\n",
    "                predictions.append((tgt_str, pred_str))\n",
    "                if pred_str == tgt_str:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    return correct / total, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:52:32.735364Z",
     "iopub.status.busy": "2025-05-20T15:52:32.734856Z",
     "iopub.status.idle": "2025-05-20T15:52:34.058407Z",
     "shell.execute_reply": "2025-05-20T15:52:34.057784Z",
     "shell.execute_reply.started": "2025-05-20T15:52:32.735340Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Attention Model Exact Match Accuracy on Test Set: 55.59%\n"
     ]
    }
   ],
   "source": [
    "attn_test_acc, attn_preds = evaluate_exact_match(\n",
    "    model_attn, test_loader, device, idx2target, target2idx[PAD_TOKEN])\n",
    "\n",
    "print(f\"‚úÖ Attention Model Exact Match Accuracy on Test Set: {attn_test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T15:32:10.321960Z",
     "iopub.status.busy": "2025-05-19T15:32:10.321310Z",
     "iopub.status.idle": "2025-05-19T15:32:10.349079Z",
     "shell.execute_reply": "2025-05-19T15:32:10.348465Z",
     "shell.execute_reply.started": "2025-05-19T15:32:10.321938Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "\n",
    "# Path(\"predictions_attention\").mkdir(exist_ok=True)\n",
    "\n",
    "# pd.DataFrame({\n",
    "#     \"Input (Roman)\": [x[0] for x in test_pairs],\n",
    "#     \"Ground Truth (Telugu)\": [g for g, _ in attn_preds],\n",
    "#     \"Prediction (Telugu)\": [p for _, p in attn_preds],\n",
    "#     \"Match\": [g == p for g, p in attn_preds]\n",
    "# }).to_csv(\"predictions_attention/test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:52:38.476681Z",
     "iopub.status.busy": "2025-05-20T15:52:38.476413Z",
     "iopub.status.idle": "2025-05-20T15:52:38.481104Z",
     "shell.execute_reply": "2025-05-20T15:52:38.480387Z",
     "shell.execute_reply.started": "2025-05-20T15:52:38.476661Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_collate(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=input2idx[PAD_TOKEN])\n",
    "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=target2idx[PAD_TOKEN])\n",
    "    return src_batch, tgt_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:52:47.592852Z",
     "iopub.status.busy": "2025-05-20T15:52:47.592582Z",
     "iopub.status.idle": "2025-05-20T15:52:50.173765Z",
     "shell.execute_reply": "2025-05-20T15:52:50.173120Z",
     "shell.execute_reply.started": "2025-05-20T15:52:47.592833Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vanilla Seq2Seq Accuracy: 54.24%\n",
      "‚úÖ Attention Seq2Seq Accuracy: 55.59%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ========= 1. Rebuild Vanilla Model =========\n",
    "best_config = {\n",
    "    \"embedding_dim\": 64,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_layers\": 3,\n",
    "    \"dropout\": 0.5,\n",
    "    \"cell_type\": \"LSTM\"\n",
    "}\n",
    "\n",
    "vanilla_encoder = Encoder(\n",
    "    vocab_size=len(input_vocab),\n",
    "    embedding_dim=best_config[\"embedding_dim\"],\n",
    "    hidden_dim=best_config[\"hidden_dim\"],\n",
    "    num_layers=best_config[\"num_layers\"],\n",
    "    dropout=best_config[\"dropout\"],\n",
    "    cell_type=best_config[\"cell_type\"]\n",
    ")\n",
    "\n",
    "vanilla_decoder = Decoder(\n",
    "    vocab_size=len(target_vocab),\n",
    "    embedding_dim=best_config[\"embedding_dim\"],\n",
    "    hidden_dim=best_config[\"hidden_dim\"],\n",
    "    num_layers=best_config[\"num_layers\"],\n",
    "    dropout=best_config[\"dropout\"],\n",
    "    cell_type=best_config[\"cell_type\"]\n",
    ")\n",
    "\n",
    "model_vanilla = Seq2Seq(vanilla_encoder, vanilla_decoder, device).to(device)\n",
    "model_vanilla.load_state_dict(torch.load(\"best_model (1).pth\", map_location=torch.device('cpu')))\n",
    "model_vanilla.eval()\n",
    "\n",
    "# ========= 2. Rebuild Attention Model =========\n",
    "best_attn_config = {\n",
    "    \"embedding_dim\": 256,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"dropout\": 0.2,\n",
    "    \"cell_type\": \"LSTM\"\n",
    "}\n",
    "\n",
    "attn_encoder = AttentionEncoder(\n",
    "    vocab_size=len(input_vocab),\n",
    "    embedding_dim=best_attn_config[\"embedding_dim\"],\n",
    "    hidden_dim=best_attn_config[\"hidden_dim\"],\n",
    "    dropout=best_attn_config[\"dropout\"],\n",
    "    cell_type=best_attn_config[\"cell_type\"]\n",
    ")\n",
    "\n",
    "attn_decoder = AttentionDecoder(\n",
    "    vocab_size=len(target_vocab),\n",
    "    embedding_dim=best_attn_config[\"embedding_dim\"],\n",
    "    enc_hidden_dim=best_attn_config[\"hidden_dim\"],\n",
    "    dec_hidden_dim=best_attn_config[\"hidden_dim\"],\n",
    "    dropout=best_attn_config[\"dropout\"],\n",
    "    cell_type=best_attn_config[\"cell_type\"]\n",
    ")\n",
    "\n",
    "model_attn = AttentionSeq2Seq(attn_encoder, attn_decoder, device).to(device)\n",
    "model_attn.load_state_dict(torch.load(\"best_model_attn (2).pth\", map_location=torch.device('cpu')))\n",
    "model_attn.eval()\n",
    "\n",
    "# ========= 3. Prepare Test Set =========\n",
    "test_input, test_target = tensorify(test_pairs, input2idx, target2idx, add_sos_eos=True)\n",
    "test_dataset = Seq2SeqDataset(test_input, test_target)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=pad_collate)\n",
    "\n",
    "# ========= 4. Detokenize Helper =========\n",
    "def detokenize(token_ids, idx2char):\n",
    "    return ''.join([idx2char[idx] for idx in token_ids if idx2char[idx] not in [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]])\n",
    "\n",
    "# ========= 5. Exact Match Evaluator =========\n",
    "def evaluate_exact_match(model, dataloader, device, idx2target, pad_idx):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    predictions = []\n",
    "    eos_idx = target2idx[EOS_TOKEN]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src, tgt, teacher_forcing_ratio=0.0)\n",
    "            if isinstance(output, tuple):  # handle (output, attn)\n",
    "                output = output[0]\n",
    "\n",
    "            preds = output.argmax(dim=-1)\n",
    "\n",
    "            for i in range(src.size(0)):\n",
    "                pred_tokens = preds[i].tolist()\n",
    "                tgt_tokens = tgt[i].tolist()\n",
    "\n",
    "                if eos_idx in pred_tokens:\n",
    "                    pred_tokens = pred_tokens[:pred_tokens.index(eos_idx)]\n",
    "                if eos_idx in tgt_tokens:\n",
    "                    tgt_tokens = tgt_tokens[:tgt_tokens.index(eos_idx)]\n",
    "\n",
    "                pred_str = detokenize(pred_tokens, idx2target)\n",
    "                tgt_str = detokenize(tgt_tokens, idx2target)\n",
    "\n",
    "                predictions.append((tgt_str, pred_str))\n",
    "                if pred_str == tgt_str:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    return correct / total, predictions\n",
    "\n",
    "# ========= 6. Evaluate Both Models =========\n",
    "vanilla_acc, vanilla_preds = evaluate_exact_match(\n",
    "    model_vanilla, test_loader, device, idx2target, target2idx[PAD_TOKEN])\n",
    "\n",
    "attn_acc, attn_preds = evaluate_exact_match(\n",
    "    model_attn, test_loader, device, idx2target, target2idx[PAD_TOKEN])\n",
    "\n",
    "print(f\"‚úÖ Vanilla Seq2Seq Accuracy: {vanilla_acc * 100:.2f}%\")\n",
    "print(f\"‚úÖ Attention Seq2Seq Accuracy: {attn_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T15:52:55.614489Z",
     "iopub.status.busy": "2025-05-20T15:52:55.613789Z",
     "iopub.status.idle": "2025-05-20T15:52:58.002118Z",
     "shell.execute_reply": "2025-05-20T15:52:58.001386Z",
     "shell.execute_reply.started": "2025-05-20T15:52:55.614465Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vanilla Seq2Seq Accuracy: 54.24%\n",
      "‚úÖ Attention Seq2Seq Accuracy: 55.59%\n"
     ]
    }
   ],
   "source": [
    "# ========= 3. Prepare Test Set =========\n",
    "test_input, test_target = tensorify(test_pairs, input2idx, target2idx, add_sos_eos=True)\n",
    "test_dataset = Seq2SeqDataset(test_input, test_target)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=pad_collate)\n",
    "\n",
    "# ========= 4. Detokenize Helper =========\n",
    "def detokenize(token_ids, idx2char):\n",
    "    return ''.join([idx2char[idx] for idx in token_ids if idx2char[idx] not in [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]])\n",
    "\n",
    "# ========= 5. Exact Match Evaluator =========\n",
    "def evaluate_exact_match(model, dataloader, device, idx2target, pad_idx):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    predictions = []\n",
    "    eos_idx = target2idx[EOS_TOKEN]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src, tgt, teacher_forcing_ratio=0.0)\n",
    "            if isinstance(output, tuple):  # handle (output, attn)\n",
    "                output = output[0]\n",
    "\n",
    "            preds = output.argmax(dim=-1)\n",
    "\n",
    "            for i in range(src.size(0)):\n",
    "                pred_tokens = preds[i].tolist()\n",
    "                tgt_tokens = tgt[i].tolist()\n",
    "\n",
    "                if eos_idx in pred_tokens:\n",
    "                    pred_tokens = pred_tokens[:pred_tokens.index(eos_idx)]\n",
    "                if eos_idx in tgt_tokens:\n",
    "                    tgt_tokens = tgt_tokens[:tgt_tokens.index(eos_idx)]\n",
    "\n",
    "                pred_str = detokenize(pred_tokens, idx2target)\n",
    "                tgt_str = detokenize(tgt_tokens, idx2target)\n",
    "\n",
    "                predictions.append((tgt_str, pred_str))\n",
    "                if pred_str == tgt_str:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    return correct / total, predictions\n",
    "\n",
    "# ========= 6. Evaluate Both Models =========\n",
    "vanilla_acc, vanilla_preds = evaluate_exact_match(\n",
    "    model_vanilla, test_loader, device, idx2target, target2idx[PAD_TOKEN])\n",
    "\n",
    "attn_acc, attn_preds = evaluate_exact_match(\n",
    "    model_attn, test_loader, device, idx2target, target2idx[PAD_TOKEN])\n",
    "\n",
    "print(f\"‚úÖ Vanilla Seq2Seq Accuracy: {vanilla_acc * 100:.2f}%\")\n",
    "print(f\"‚úÖ Attention Seq2Seq Accuracy: {attn_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T13:41:24.202838Z",
     "iopub.status.busy": "2025-05-20T13:41:24.202168Z",
     "iopub.status.idle": "2025-05-20T13:41:24.225169Z",
     "shell.execute_reply": "2025-05-20T13:41:24.224277Z",
     "shell.execute_reply.started": "2025-05-20T13:41:24.202817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Path(\"predictions_vanilla\").mkdir(exist_ok=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Input (Roman)\": [x[0] for x in test_pairs],\n",
    "    \"Ground Truth (Telugu)\": [gt for gt, _ in vanilla_preds],\n",
    "    \"Vanilla Prediction\": [pred for _, pred in vanilla_preds],\n",
    "    \"Match\": [gt == pred for gt, pred in vanilla_preds]\n",
    "}).to_csv(\"predictions_vanilla/test_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T13:41:27.796675Z",
     "iopub.status.busy": "2025-05-20T13:41:27.795958Z",
     "iopub.status.idle": "2025-05-20T13:41:27.815284Z",
     "shell.execute_reply": "2025-05-20T13:41:27.814768Z",
     "shell.execute_reply.started": "2025-05-20T13:41:27.796641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Path(\"predictions_attention\").mkdir(exist_ok=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Input (Roman)\": [x[0] for x in test_pairs],\n",
    "    \"Ground Truth (Telugu)\": [gt for gt, _ in attn_preds],\n",
    "    \"Attention Prediction\": [pred for _, pred in attn_preds],\n",
    "    \"Match\": [gt == pred for gt, pred in attn_preds]\n",
    "}).to_csv(\"predictions_attention/test_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T13:42:46.460095Z",
     "iopub.status.busy": "2025-05-20T13:42:46.459403Z",
     "iopub.status.idle": "2025-05-20T13:42:46.488716Z",
     "shell.execute_reply": "2025-05-20T13:42:46.488015Z",
     "shell.execute_reply.started": "2025-05-20T13:42:46.460064Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fixed cases by attention: 673\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input (Roman)</th>\n",
       "      <th>Ground Truth</th>\n",
       "      <th>Vanilla Prediction</th>\n",
       "      <th>Attention Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>polaalaku</td>\n",
       "      <td>‡∞™‡±ä‡∞≤‡∞æ‡∞≤‡∞ï‡±Å</td>\n",
       "      <td>‡∞™‡±ä‡∞≤‡∞æ‡∞ï‡±Å</td>\n",
       "      <td>‡∞™‡±ä‡∞≤‡∞æ‡∞≤‡∞ï‡±Å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>prodakshans</td>\n",
       "      <td>‡∞™‡±ç‡∞∞‡±ä‡∞°‡∞ï‡±ç‡∞∑‡∞®‡±ç‡∞∏‡±ç</td>\n",
       "      <td>‡∞™‡±ç‡∞∞‡±ä‡∞¶‡±ç‡∞ï‡∞∂‡∞Ç‡∞∏‡±ç</td>\n",
       "      <td>‡∞™‡±ç‡∞∞‡±ä‡∞°‡∞ï‡±ç‡∞∑‡∞®‡±ç‡∞∏‡±ç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>abhimananiki</td>\n",
       "      <td>‡∞Ö‡∞≠‡∞ø‡∞Æ‡∞æ‡∞®‡∞æ‡∞®‡∞ø‡∞ï‡∞ø</td>\n",
       "      <td>‡∞Ö‡∞≠‡∞ø‡∞Æ‡∞®‡∞æ‡∞®‡∞ø‡∞ï‡∞ø</td>\n",
       "      <td>‡∞Ö‡∞≠‡∞ø‡∞Æ‡∞æ‡∞®‡∞æ‡∞®‡∞ø‡∞ï‡∞ø</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>prasadaravu</td>\n",
       "      <td>‡∞™‡±ç‡∞∞‡∞∏‡∞æ‡∞¶‡∞∞‡∞æ‡∞µ‡±Å</td>\n",
       "      <td>‡∞™‡±ç‡∞∞‡∞∏‡∞æ‡∞¶‡∞æ‡∞∞‡∞µ‡∞µ‡±Å</td>\n",
       "      <td>‡∞™‡±ç‡∞∞‡∞∏‡∞æ‡∞¶‡∞∞‡∞æ‡∞µ‡±Å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>dhaasooham</td>\n",
       "      <td>‡∞¶‡∞æ‡∞∏‡±ã‡∞π‡∞Ç</td>\n",
       "      <td>‡∞¶‡∞æ‡∞∂‡±ã‡∞π‡∞Ç</td>\n",
       "      <td>‡∞¶‡∞æ‡∞∏‡±ã‡∞π‡∞Ç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>ghaniibhavinchina</td>\n",
       "      <td>‡∞ò‡∞®‡±Ä‡∞≠‡∞µ‡∞ø‡∞Ç‡∞ö‡∞ø‡∞®</td>\n",
       "      <td>‡∞ò‡∞®‡∞ø‡∞≠‡∞µ‡∞ø‡∞Ç‡∞ö‡∞ø‡∞®</td>\n",
       "      <td>‡∞ò‡∞®‡±Ä‡∞≠‡∞µ‡∞ø‡∞Ç‡∞ö‡∞ø‡∞®</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>sachivaalayam</td>\n",
       "      <td>‡∞∏‡∞ö‡∞ø‡∞µ‡∞æ‡∞≤‡∞Ø‡∞Ç</td>\n",
       "      <td>‡∞∂‡∞ö‡±ç‡∞ö‡∞ø‡∞µ‡∞æ‡∞≤‡∞Ø‡∞Ç</td>\n",
       "      <td>‡∞∏‡∞ö‡∞ø‡∞µ‡∞æ‡∞≤‡∞Ø‡∞Ç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>mickei</td>\n",
       "      <td>‡∞Æ‡∞ø‡∞ï‡±ç‡∞ï‡±Ä</td>\n",
       "      <td>‡∞Æ‡∞ø‡∞ï‡±Ä</td>\n",
       "      <td>‡∞Æ‡∞ø‡∞ï‡±ç‡∞ï‡±Ä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>pramukhulaina</td>\n",
       "      <td>‡∞™‡±ç‡∞∞‡∞Æ‡±Å‡∞ñ‡±Å‡∞≤‡±à‡∞®</td>\n",
       "      <td>‡∞™‡±ç‡∞∞‡∞æ‡∞Æ‡±Å‡∞ñ‡±Å‡∞≤‡±à‡∞®</td>\n",
       "      <td>‡∞™‡±ç‡∞∞‡∞Æ‡±Å‡∞ñ‡±Å‡∞≤‡±à‡∞®</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>baetaalhudu</td>\n",
       "      <td>‡∞¨‡±á‡∞§‡∞æ‡∞≥‡±Å‡∞°‡±Å</td>\n",
       "      <td>‡∞¨‡±á‡∞§‡∞æ‡∞≤‡±Å‡∞°‡±Å</td>\n",
       "      <td>‡∞¨‡±á‡∞§‡∞æ‡∞≥‡±Å‡∞°‡±Å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>varasaga</td>\n",
       "      <td>‡∞µ‡∞∞‡∞∏‡∞ó‡∞æ</td>\n",
       "      <td>‡∞µ‡∞æ‡∞∞‡∞∏‡∞ó‡∞æ</td>\n",
       "      <td>‡∞µ‡∞∞‡∞∏‡∞ó‡∞æ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>madraasuku</td>\n",
       "      <td>‡∞Æ‡∞¶‡±ç‡∞∞‡∞æ‡∞∏‡±Å‡∞ï‡±Å</td>\n",
       "      <td>‡∞Æ‡∞¶‡±ç‡∞∞‡∞∏‡±Å‡∞ï‡±Å</td>\n",
       "      <td>‡∞Æ‡∞¶‡±ç‡∞∞‡∞æ‡∞∏‡±Å‡∞ï‡±Å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>dongalaa</td>\n",
       "      <td>‡∞¶‡±ä‡∞Ç‡∞ó‡∞≤‡∞æ</td>\n",
       "      <td>‡∞¶‡±ä‡∞Ç‡∞ó‡∞≥‡∞æ</td>\n",
       "      <td>‡∞¶‡±ä‡∞Ç‡∞ó‡∞≤‡∞æ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>sreyobhilaashi</td>\n",
       "      <td>‡∞∂‡±ç‡∞∞‡±á‡∞Ø‡±ã‡∞≠‡∞ø‡∞≤‡∞æ‡∞∑‡∞ø</td>\n",
       "      <td>‡∞∂‡±ç‡∞∞‡±á‡∞Ø‡±ã‡∞π‡∞ø‡∞∏‡∞æ‡∞≤‡∞ø</td>\n",
       "      <td>‡∞∂‡±ç‡∞∞‡±á‡∞Ø‡±ã‡∞≠‡∞ø‡∞≤‡∞æ‡∞∑‡∞ø</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>states</td>\n",
       "      <td>‡∞∏‡±ç‡∞ü‡±á‡∞ü‡±ç‡∞∏‡±ç</td>\n",
       "      <td>‡∞∏‡±ç‡∞ü‡±á‡∞∑‡±ç‡∞∏‡±ç</td>\n",
       "      <td>‡∞∏‡±ç‡∞ü‡±á‡∞ü‡±ç‡∞∏‡±ç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>ballapai</td>\n",
       "      <td>‡∞¨‡∞≤‡±ç‡∞≤‡∞™‡±à</td>\n",
       "      <td>‡∞¨‡∞≥‡±ç‡∞≤‡∞™‡±à</td>\n",
       "      <td>‡∞¨‡∞≤‡±ç‡∞≤‡∞™‡±à</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>chainiiyula</td>\n",
       "      <td>‡∞ö‡±à‡∞®‡±Ä‡∞Ø‡±Å‡∞≤</td>\n",
       "      <td>‡∞ö‡±à‡∞®‡∞ø‡∞Ø‡±Å‡∞≤</td>\n",
       "      <td>‡∞ö‡±à‡∞®‡±Ä‡∞Ø‡±Å‡∞≤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>cheeyagaladu</td>\n",
       "      <td>‡∞ö‡±á‡∞Ø‡∞ó‡∞≤‡∞¶‡±Å</td>\n",
       "      <td>‡∞ö‡±á‡∞Ø‡∞ó‡∞≤‡∞æ‡∞°‡±Å</td>\n",
       "      <td>‡∞ö‡±á‡∞Ø‡∞ó‡∞≤‡∞¶‡±Å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>yaasid</td>\n",
       "      <td>‡∞Ø‡∞æ‡∞∏‡∞ø‡∞°‡±ç</td>\n",
       "      <td>‡∞Ø‡∞æ‡∞∏‡∞ø‡∞¶‡±ç</td>\n",
       "      <td>‡∞Ø‡∞æ‡∞∏‡∞ø‡∞°‡±ç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>siitaramayya</td>\n",
       "      <td>‡∞∏‡±Ä‡∞§‡∞æ‡∞∞‡∞æ‡∞Æ‡∞Ø‡±ç‡∞Ø</td>\n",
       "      <td>‡∞∏‡±Ä‡∞§‡∞æ‡∞∞‡∞Æ‡∞Ø‡±ç‡∞Ø</td>\n",
       "      <td>‡∞∏‡±Ä‡∞§‡∞æ‡∞∞‡∞æ‡∞Æ‡∞Ø‡±ç‡∞Ø</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Input (Roman)  Ground Truth Vanilla Prediction Attention Prediction\n",
       "359          polaalaku       ‡∞™‡±ä‡∞≤‡∞æ‡∞≤‡∞ï‡±Å             ‡∞™‡±ä‡∞≤‡∞æ‡∞ï‡±Å              ‡∞™‡±ä‡∞≤‡∞æ‡∞≤‡∞ï‡±Å\n",
       "402        prodakshans  ‡∞™‡±ç‡∞∞‡±ä‡∞°‡∞ï‡±ç‡∞∑‡∞®‡±ç‡∞∏‡±ç        ‡∞™‡±ç‡∞∞‡±ä‡∞¶‡±ç‡∞ï‡∞∂‡∞Ç‡∞∏‡±ç         ‡∞™‡±ç‡∞∞‡±ä‡∞°‡∞ï‡±ç‡∞∑‡∞®‡±ç‡∞∏‡±ç\n",
       "32        abhimananiki   ‡∞Ö‡∞≠‡∞ø‡∞Æ‡∞æ‡∞®‡∞æ‡∞®‡∞ø‡∞ï‡∞ø         ‡∞Ö‡∞≠‡∞ø‡∞Æ‡∞®‡∞æ‡∞®‡∞ø‡∞ï‡∞ø          ‡∞Ö‡∞≠‡∞ø‡∞Æ‡∞æ‡∞®‡∞æ‡∞®‡∞ø‡∞ï‡∞ø\n",
       "393        prasadaravu    ‡∞™‡±ç‡∞∞‡∞∏‡∞æ‡∞¶‡∞∞‡∞æ‡∞µ‡±Å        ‡∞™‡±ç‡∞∞‡∞∏‡∞æ‡∞¶‡∞æ‡∞∞‡∞µ‡∞µ‡±Å           ‡∞™‡±ç‡∞∞‡∞∏‡∞æ‡∞¶‡∞∞‡∞æ‡∞µ‡±Å\n",
       "265         dhaasooham        ‡∞¶‡∞æ‡∞∏‡±ã‡∞π‡∞Ç             ‡∞¶‡∞æ‡∞∂‡±ã‡∞π‡∞Ç               ‡∞¶‡∞æ‡∞∏‡±ã‡∞π‡∞Ç\n",
       "181  ghaniibhavinchina    ‡∞ò‡∞®‡±Ä‡∞≠‡∞µ‡∞ø‡∞Ç‡∞ö‡∞ø‡∞®         ‡∞ò‡∞®‡∞ø‡∞≠‡∞µ‡∞ø‡∞Ç‡∞ö‡∞ø‡∞®           ‡∞ò‡∞®‡±Ä‡∞≠‡∞µ‡∞ø‡∞Ç‡∞ö‡∞ø‡∞®\n",
       "623      sachivaalayam      ‡∞∏‡∞ö‡∞ø‡∞µ‡∞æ‡∞≤‡∞Ø‡∞Ç         ‡∞∂‡∞ö‡±ç‡∞ö‡∞ø‡∞µ‡∞æ‡∞≤‡∞Ø‡∞Ç             ‡∞∏‡∞ö‡∞ø‡∞µ‡∞æ‡∞≤‡∞Ø‡∞Ç\n",
       "482             mickei        ‡∞Æ‡∞ø‡∞ï‡±ç‡∞ï‡±Ä               ‡∞Æ‡∞ø‡∞ï‡±Ä               ‡∞Æ‡∞ø‡∞ï‡±ç‡∞ï‡±Ä\n",
       "382      pramukhulaina    ‡∞™‡±ç‡∞∞‡∞Æ‡±Å‡∞ñ‡±Å‡∞≤‡±à‡∞®        ‡∞™‡±ç‡∞∞‡∞æ‡∞Æ‡±Å‡∞ñ‡±Å‡∞≤‡±à‡∞®           ‡∞™‡±ç‡∞∞‡∞Æ‡±Å‡∞ñ‡±Å‡∞≤‡±à‡∞®\n",
       "437        baetaalhudu      ‡∞¨‡±á‡∞§‡∞æ‡∞≥‡±Å‡∞°‡±Å           ‡∞¨‡±á‡∞§‡∞æ‡∞≤‡±Å‡∞°‡±Å             ‡∞¨‡±á‡∞§‡∞æ‡∞≥‡±Å‡∞°‡±Å\n",
       "550           varasaga         ‡∞µ‡∞∞‡∞∏‡∞ó‡∞æ             ‡∞µ‡∞æ‡∞∞‡∞∏‡∞ó‡∞æ                ‡∞µ‡∞∞‡∞∏‡∞ó‡∞æ\n",
       "459         madraasuku     ‡∞Æ‡∞¶‡±ç‡∞∞‡∞æ‡∞∏‡±Å‡∞ï‡±Å           ‡∞Æ‡∞¶‡±ç‡∞∞‡∞∏‡±Å‡∞ï‡±Å            ‡∞Æ‡∞¶‡±ç‡∞∞‡∞æ‡∞∏‡±Å‡∞ï‡±Å\n",
       "280           dongalaa        ‡∞¶‡±ä‡∞Ç‡∞ó‡∞≤‡∞æ             ‡∞¶‡±ä‡∞Ç‡∞ó‡∞≥‡∞æ               ‡∞¶‡±ä‡∞Ç‡∞ó‡∞≤‡∞æ\n",
       "614     sreyobhilaashi  ‡∞∂‡±ç‡∞∞‡±á‡∞Ø‡±ã‡∞≠‡∞ø‡∞≤‡∞æ‡∞∑‡∞ø       ‡∞∂‡±ç‡∞∞‡±á‡∞Ø‡±ã‡∞π‡∞ø‡∞∏‡∞æ‡∞≤‡∞ø         ‡∞∂‡±ç‡∞∞‡±á‡∞Ø‡±ã‡∞≠‡∞ø‡∞≤‡∞æ‡∞∑‡∞ø\n",
       "659             states      ‡∞∏‡±ç‡∞ü‡±á‡∞ü‡±ç‡∞∏‡±ç           ‡∞∏‡±ç‡∞ü‡±á‡∞∑‡±ç‡∞∏‡±ç             ‡∞∏‡±ç‡∞ü‡±á‡∞ü‡±ç‡∞∏‡±ç\n",
       "424           ballapai        ‡∞¨‡∞≤‡±ç‡∞≤‡∞™‡±à             ‡∞¨‡∞≥‡±ç‡∞≤‡∞™‡±à               ‡∞¨‡∞≤‡±ç‡∞≤‡∞™‡±à\n",
       "212        chainiiyula       ‡∞ö‡±à‡∞®‡±Ä‡∞Ø‡±Å‡∞≤            ‡∞ö‡±à‡∞®‡∞ø‡∞Ø‡±Å‡∞≤              ‡∞ö‡±à‡∞®‡±Ä‡∞Ø‡±Å‡∞≤\n",
       "204       cheeyagaladu       ‡∞ö‡±á‡∞Ø‡∞ó‡∞≤‡∞¶‡±Å           ‡∞ö‡±á‡∞Ø‡∞ó‡∞≤‡∞æ‡∞°‡±Å              ‡∞ö‡±á‡∞Ø‡∞ó‡∞≤‡∞¶‡±Å\n",
       "507             yaasid        ‡∞Ø‡∞æ‡∞∏‡∞ø‡∞°‡±ç             ‡∞Ø‡∞æ‡∞∏‡∞ø‡∞¶‡±ç               ‡∞Ø‡∞æ‡∞∏‡∞ø‡∞°‡±ç\n",
       "644       siitaramayya    ‡∞∏‡±Ä‡∞§‡∞æ‡∞∞‡∞æ‡∞Æ‡∞Ø‡±ç‡∞Ø          ‡∞∏‡±Ä‡∞§‡∞æ‡∞∞‡∞Æ‡∞Ø‡±ç‡∞Ø           ‡∞∏‡±Ä‡∞§‡∞æ‡∞∞‡∞æ‡∞Æ‡∞Ø‡±ç‡∞Ø"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison = []\n",
    "\n",
    "for i, (v, a) in enumerate(zip(vanilla_preds, attn_preds)):\n",
    "    truth = v[0]\n",
    "    if v[1] != truth and a[1] == truth:  # fixed by attention\n",
    "        comparison.append({\n",
    "            \"Input (Roman)\": test_pairs[i][0],\n",
    "            \"Ground Truth\": truth,\n",
    "            \"Vanilla Prediction\": v[1],\n",
    "            \"Attention Prediction\": a[1]\n",
    "        })\n",
    "\n",
    "df_fixed = pd.DataFrame(comparison)\n",
    "df_fixed.to_csv(\"predictions_attention/fixed_by_attention.csv\", index=False)\n",
    "\n",
    "print(f\"‚úÖ Fixed cases by attention: {len(df_fixed)}\")\n",
    "df_fixed.sample(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T13:46:01.244950Z",
     "iopub.status.busy": "2025-05-20T13:46:01.244674Z",
     "iopub.status.idle": "2025-05-20T13:46:01.251449Z",
     "shell.execute_reply": "2025-05-20T13:46:01.250710Z",
     "shell.execute_reply.started": "2025-05-20T13:46:01.244929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def debug_attention(model, test_pairs, input2idx, target2idx, idx2target, device):\n",
    "    model.eval()\n",
    "\n",
    "    # Pick a test input\n",
    "    input_str = \"nenu\"  # or test_pairs[0][0]\n",
    "    print(f\"‚ñ∂ Input string: {input_str}\")\n",
    "\n",
    "    # Convert to tensor\n",
    "    src_tensor = torch.tensor([tokenize(input_str, input2idx, add_sos_eos=True)], device=device)\n",
    "    max_tgt_len = 10\n",
    "    tgt_tensor = torch.tensor([[target2idx[SOS_TOKEN]] + [target2idx[PAD_TOKEN]] * (max_tgt_len - 1)], device=device)\n",
    "\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output, attentions = model(src_tensor, tgt_tensor, teacher_forcing_ratio=0.0)\n",
    "\n",
    "    # Output shapes\n",
    "    print(\"‚úÖ Attention shape:\", attentions.shape)  # should be [1, tgt_len, src_len]\n",
    "    print(\"‚úÖ Output shape:\", output.shape)\n",
    "\n",
    "    # Attention matrix\n",
    "    attn_matrix = attentions[0].cpu().numpy()\n",
    "    print(\"‚úÖ Raw Attention values:\\n\", attn_matrix)\n",
    "\n",
    "    # Show stats\n",
    "    print(\"Min:\", attn_matrix.min(), \"Max:\", attn_matrix.max(), \"Mean:\", attn_matrix.mean())\n",
    "\n",
    "    # Optional: Visualize 1st row of attention\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    plt.plot(attn_matrix[0])\n",
    "    plt.title(\"Attention Weights for First Output Token\")\n",
    "    plt.xlabel(\"Input Sequence Positions\")\n",
    "    plt.ylabel(\"Weight\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T13:46:02.914084Z",
     "iopub.status.busy": "2025-05-20T13:46:02.913423Z",
     "iopub.status.idle": "2025-05-20T13:46:03.050313Z",
     "shell.execute_reply": "2025-05-20T13:46:03.049710Z",
     "shell.execute_reply.started": "2025-05-20T13:46:02.914036Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Input string: nenu\n",
      "‚úÖ Attention shape: torch.Size([1, 10, 6])\n",
      "‚úÖ Output shape: torch.Size([1, 10, 66])\n",
      "‚úÖ Raw Attention values:\n",
      " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00]\n",
      " [2.3990932e-01 2.2384711e-01 1.1174922e-01 3.5822281e-01 6.1696835e-02\n",
      "  4.5747492e-03]\n",
      " [1.2576547e-02 1.4270236e-03 4.2789510e-01 5.3862864e-01 1.7826190e-02\n",
      "  1.6465121e-03]\n",
      " [1.3398418e-03 5.9552349e-05 1.5955931e-03 2.0497811e-01 6.9666666e-01\n",
      "  9.5360219e-02]\n",
      " [2.9668723e-09 2.3074610e-10 3.1817935e-09 5.5043092e-05 2.6342756e-01\n",
      "  7.3651737e-01]\n",
      " [5.5622580e-14 5.1435686e-15 1.5775145e-12 2.9483042e-08 4.7460021e-04\n",
      "  9.9952543e-01]\n",
      " [1.8058872e-15 3.9792425e-14 6.3814560e-15 4.6508290e-11 2.3921593e-05\n",
      "  9.9997604e-01]\n",
      " [1.9676676e-13 5.5233249e-13 4.4157763e-12 1.1916016e-09 1.4261408e-04\n",
      "  9.9985731e-01]\n",
      " [9.7407022e-13 2.4224914e-11 2.9306598e-12 1.6690971e-09 7.3733377e-06\n",
      "  9.9999261e-01]\n",
      " [2.8651903e-10 2.5042848e-09 5.5206533e-09 1.4206474e-07 3.0984887e-04\n",
      "  9.9969006e-01]]\n",
      "Min: 0.0 Max: 0.9999926 Mean: 0.15\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "debug_attention(model_attn, test_pairs, input2idx, target2idx, idx2target, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T13:48:04.770075Z",
     "iopub.status.busy": "2025-05-20T13:48:04.769524Z",
     "iopub.status.idle": "2025-05-20T13:48:11.490969Z",
     "shell.execute_reply": "2025-05-20T13:48:11.490289Z",
     "shell.execute_reply.started": "2025-05-20T13:48:04.770030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"DA6401_Assignment_3\", name=\"attention_heatmaps_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T13:54:20.794108Z",
     "iopub.status.busy": "2025-05-20T13:54:20.793386Z",
     "iopub.status.idle": "2025-05-20T13:54:20.814457Z",
     "shell.execute_reply": "2025-05-20T13:54:20.813703Z",
     "shell.execute_reply.started": "2025-05-20T13:54:20.794082Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Registered Telugu font globally: Noto Sans Telugu\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.font_manager import fontManager, FontProperties\n",
    "\n",
    "# Point to your Telugu font\n",
    "telugu_font_path = \"Noto_Sans_Telugu/static/NotoSansTelugu-Black.ttf\"\n",
    "\n",
    "# Register and set as default\n",
    "fontManager.addfont(telugu_font_path)\n",
    "telugu_font = FontProperties(fname=telugu_font_path)\n",
    "mpl.rcParams['font.family'] = telugu_font.get_name()\n",
    "\n",
    "print(\"‚úÖ Registered Telugu font globally:\", telugu_font.get_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T13:51:51.417718Z",
     "iopub.status.busy": "2025-05-20T13:51:51.417096Z",
     "iopub.status.idle": "2025-05-20T13:51:51.422794Z",
     "shell.execute_reply": "2025-05-20T13:51:51.422030Z",
     "shell.execute_reply.started": "2025-05-20T13:51:51.417690Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.font_manager import FontProperties\n",
    "telugu_font = FontProperties(fname=\"Noto_Sans_Telugu/static/NotoSansTelugu-Black.ttf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T13:51:59.973593Z",
     "iopub.status.busy": "2025-05-20T13:51:59.973326Z",
     "iopub.status.idle": "2025-05-20T13:52:00.129462Z",
     "shell.execute_reply": "2025-05-20T13:52:00.128699Z",
     "shell.execute_reply.started": "2025-05-20T13:51:59.973573Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Attention Heatmap')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xticks(fontproperties=telugu_font, rotation=0, fontsize=12)\n",
    "plt.yticks(fontproperties=telugu_font, rotation=0, fontsize=12)\n",
    "plt.xlabel(\"Input Characters\", fontproperties=telugu_font, fontsize=14)\n",
    "plt.ylabel(\"Predicted Characters\", fontproperties=telugu_font, fontsize=14)\n",
    "plt.title(\"Attention Heatmap\", fontproperties=telugu_font, fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T13:52:23.466695Z",
     "iopub.status.busy": "2025-05-20T13:52:23.466412Z",
     "iopub.status.idle": "2025-05-20T13:52:23.471733Z",
     "shell.execute_reply": "2025-05-20T13:52:23.470942Z",
     "shell.execute_reply.started": "2025-05-20T13:52:23.466675Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "telugu_font = FontProperties(fname=\"Noto_Sans_Telugu/static/NotoSansTelugu-Black.ttf\")\n",
    "rcParams['font.family'] = telugu_font.get_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:38:26.801611Z",
     "iopub.status.busy": "2025-05-20T16:38:26.800853Z",
     "iopub.status.idle": "2025-05-20T16:38:26.838163Z",
     "shell.execute_reply": "2025-05-20T16:38:26.837527Z",
     "shell.execute_reply.started": "2025-05-20T16:38:26.801575Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4,2))\n",
    "plt.text(0.5, 0.5, \"‡∞®‡±á‡∞®‡±Å\", fontproperties=telugu_font, fontsize=24, ha='center')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:45:13.061705Z",
     "iopub.status.busy": "2025-05-20T16:45:13.060985Z",
     "iopub.status.idle": "2025-05-20T16:45:13.068804Z",
     "shell.execute_reply": "2025-05-20T16:45:13.068059Z",
     "shell.execute_reply.started": "2025-05-20T16:45:13.061672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_attention_example(model, input_text, input2idx, target2idx, idx2target, device, max_len=30):\n",
    "    model.eval()\n",
    "    src_tensor = torch.tensor([ [input2idx.get(c, input2idx['<pad>']) for c in [\"<sos>\"]+list(input_text)+[\"<eos>\"]]], device=device)\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "    # Generate output sequence and collect attention\n",
    "    input_tokens = [\"<sos>\"] + list(input_text) + [\"<eos>\"]\n",
    "    pred_tokens, attns = [], []\n",
    "    input_token = torch.tensor([target2idx[\"<sos>\"]], device=device)\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output, hidden, attn_weights = model.decoder(input_token, hidden, encoder_outputs)\n",
    "            top1 = output.argmax(1).item()\n",
    "            pred_tokens.append(idx2target[top1])\n",
    "            attns.append(attn_weights.squeeze(0).cpu().numpy())\n",
    "        if idx2target[top1] == \"<eos>\":\n",
    "            break\n",
    "        input_token = torch.tensor([top1], device=device)\n",
    "    return {\n",
    "        \"input\": input_tokens,\n",
    "        \"output\": pred_tokens,\n",
    "        \"attentions\": attns\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:47:01.369555Z",
     "iopub.status.busy": "2025-05-20T16:47:01.369190Z",
     "iopub.status.idle": "2025-05-20T16:47:01.383871Z",
     "shell.execute_reply": "2025-05-20T16:47:01.383012Z",
     "shell.execute_reply.started": "2025-05-20T16:47:01.369532Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "example = get_attention_example(model_attn, \"nenu\", input2idx, target2idx, idx2target, device)\n",
    "# Convert all attention vectors from np.ndarray to list\n",
    "for i in range(len(example[\"attentions\"])):\n",
    "    example[\"attentions\"][i] = example[\"attentions\"][i].tolist()\n",
    "with open(\"attention_example.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(example, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization saved to: connectivity_attention.html\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def write_attention_html(example, filename=\"attention_vis.html\"):\n",
    "    html = f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Seq2Seq Attention Connectivity</title>\n",
    "    <style>\n",
    "      .tok {{padding:4px 6px;margin:2px;border-radius:4px;\n",
    "             display:inline-block;font-family:monospace;\n",
    "             background:#eee;cursor:pointer}}\n",
    "      .hi {{background:rgba(0,200,0,.5)}}\n",
    "    </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h2>Interactive Attention (hover output &rarr; see input focus)</h2>\n",
    "    <div><b>Input:</b> <span id=\"in\"></span></div><br>\n",
    "    <div><b>Output:</b> <span id=\"out\"></span></div>\n",
    "\n",
    "    <script>\n",
    "      const data = {json.dumps(example, ensure_ascii=False)};\n",
    "      const inDiv  = document.getElementById(\"in\");\n",
    "      const outDiv = document.getElementById(\"out\");\n",
    "      // Draw input tokens (skip <sos>/<eos>)\n",
    "      const shownInputs = data.input.slice(1, -1);\n",
    "      shownInputs.forEach((ch,i)=>{{\n",
    "        const s=document.createElement(\"span\");\n",
    "        s.textContent=ch; s.className=\"tok\"; s.id=\"in-\"+i;\n",
    "        inDiv.appendChild(s);\n",
    "      }});\n",
    "      // Draw output tokens\n",
    "      data.output.forEach((ch,i)=>{{\n",
    "        const s=document.createElement(\"span\");\n",
    "        s.textContent=ch; s.className=\"tok\";\n",
    "        s.onmouseenter=()=>{{\n",
    "          shownInputs.forEach((_,j)=>{{\n",
    "            document.getElementById(\"in-\"+j).classList.remove(\"hi\");\n",
    "          }});\n",
    "          const att = data.attentions[i];\n",
    "          const m = Math.max(...att);\n",
    "          att.forEach((v,j)=>{{\n",
    "            if(v >= 0.5*m)\n",
    "              document.getElementById(\"in-\"+j).classList.add(\"hi\");\n",
    "          }});\n",
    "        }};\n",
    "        s.onmouseleave=()=>{{\n",
    "          shownInputs.forEach((_,j)=>{{\n",
    "            document.getElementById(\"in-\"+j).classList.remove(\"hi\");\n",
    "          }});\n",
    "        }};\n",
    "        outDiv.appendChild(s);\n",
    "      }});\n",
    "    </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "    print(f\"Visualization saved to: {filename}\")\n",
    "\n",
    "# Example use (put your model's example here):\n",
    "example = {\n",
    "    \"input\": [\"<sos>\", \"h\", \"e\", \"l\", \"l\", \"o\", \"<eos>\"],\n",
    "    \"output\": [\"‡∞π\", \"‡∞≤\", \"‡±ã\", \"<eos>\"],\n",
    "    \"attentions\": [\n",
    "        [0.01, 0.05, 0.1, 0.1, 0.1, 0.6],  # \"‡∞π\"\n",
    "        [0.01, 0.2, 0.2, 0.3, 0.2, 0.09],  # \"‡∞≤\"\n",
    "        [0.02, 0.1, 0.5, 0.25, 0.08, 0.05], # \"‡±ã\"\n",
    "        [0, 0, 0, 0, 0, 1.0],              # <eos>\n",
    "    ]\n",
    "}\n",
    "\n",
    "write_attention_html(example, \"connectivity_attention.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_attention_example(model, input_text, input2idx, target2idx, idx2target, device, max_len=30):\n",
    "    \"\"\"\n",
    "    Run the model on a single input and extract:\n",
    "    - input: list of input characters (incl. <sos>/<eos>)\n",
    "    - output: list of predicted output characters (incl. <eos>)\n",
    "    - attentions: list of attention arrays (output_pos √ó input_pos)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode input (with <sos> and <eos>)\n",
    "        src_indices = [input2idx.get(c, input2idx['<pad>']) for c in input_text]\n",
    "        src_tensor = torch.tensor([[input2idx['<sos>']] + src_indices + [input2idx['<eos>']]], device=device)\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "        input_chars = ['<sos>'] + list(input_text) + ['<eos>']\n",
    "\n",
    "        # Decoding\n",
    "        pred_chars = []\n",
    "        attentions = []\n",
    "        dec_input = torch.tensor([target2idx['<sos>']], device=device)\n",
    "        for _ in range(max_len):\n",
    "            out, hidden, attn_weights = model.decoder(dec_input, hidden, encoder_outputs)\n",
    "            top1 = out.argmax(1).item()\n",
    "            pred_chars.append(idx2target[top1])\n",
    "            attentions.append(attn_weights.squeeze(0).cpu().numpy().tolist())\n",
    "            if idx2target[top1] == '<eos>':\n",
    "                break\n",
    "            dec_input = torch.tensor([top1], device=device)\n",
    "        return {\n",
    "            \"input\": input_chars,\n",
    "            \"output\": pred_chars,\n",
    "            \"attentions\": attentions\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_attention_html_gradient(example, filename=\"attention_vis_gradient.html\"):\n",
    "    html = f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Seq2Seq Attention Connectivity (Gradient)</title>\n",
    "    <style>\n",
    "      .tok {{\n",
    "        padding:4px 6px;margin:2px;border-radius:4px;\n",
    "        display:inline-block;font-family:monospace;\n",
    "        background:#eee;cursor:pointer;transition:background 0.2s;\n",
    "      }}\n",
    "      .tok.hi {{\n",
    "        background:rgba(0,200,0,0.5);\n",
    "      }}\n",
    "    </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h2>Interactive Attention (hover output ‚Üí see input focus)</h2>\n",
    "    <div><b>Input:</b> <span id=\"in\"></span></div><br>\n",
    "    <div><b>Output:</b> <span id=\"out\"></span></div>\n",
    "\n",
    "    <script>\n",
    "      const data = {json.dumps(example, ensure_ascii=False)};\n",
    "      const inDiv  = document.getElementById(\"in\");\n",
    "      const outDiv = document.getElementById(\"out\");\n",
    "\n",
    "      // Draw input tokens (skip <sos>/<eos> for display)\n",
    "      const shownInputs = data.input.slice(1, -1);\n",
    "\n",
    "      function resetInputColors() {{\n",
    "        shownInputs.forEach((_, j) => {{\n",
    "          const elem = document.getElementById(\"in-\" + j);\n",
    "          elem.style.background = \"#eee\";\n",
    "        }});\n",
    "      }}\n",
    "\n",
    "      shownInputs.forEach((ch, i) => {{\n",
    "        const s = document.createElement(\"span\");\n",
    "        s.textContent = ch;\n",
    "        s.className = \"tok\";\n",
    "        s.id = \"in-\" + i;\n",
    "        inDiv.appendChild(s);\n",
    "      }});\n",
    "\n",
    "      // Draw output tokens\n",
    "      data.output.forEach((ch, i) => {{\n",
    "        const s = document.createElement(\"span\");\n",
    "        s.textContent = ch;\n",
    "        s.className = \"tok\";\n",
    "        s.onmouseenter = () => {{\n",
    "          resetInputColors();\n",
    "          // Get attention weights for this output char\n",
    "          const att = data.attentions[i];\n",
    "          // Find max for normalization\n",
    "          const m = Math.max(...att);\n",
    "          att.forEach((v, j) => {{\n",
    "            const elem = document.getElementById(\"in-\" + j);\n",
    "            // Linear green gradient (can change rgba for other colors)\n",
    "            elem.style.background = `rgba(0,200,0,${(v/m).toFixed(2)})`;\n",
    "          }});\n",
    "        }};\n",
    "        s.onmouseleave = () => {{\n",
    "          resetInputColors();\n",
    "        }};\n",
    "        outDiv.appendChild(s);\n",
    "      }});\n",
    "    </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "    print(f\"Visualization with gradient saved to: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a test word, e.g. \"nenu\"\n",
    "example = get_attention_example(\n",
    "    model_attn,           # Your loaded model\n",
    "    \"sitaramayya\",               # Input string\n",
    "    input2idx,            # Dict: char -> index (input vocab)\n",
    "    target2idx,           # Dict: char -> index (target vocab)\n",
    "    idx2target,           # Dict: index -> char (target vocab)\n",
    "    device                # \"cuda\" or \"cpu\"\n",
    ")\n",
    "\n",
    "# Save and view visualization\n",
    "#write_attention_html(example, \"connectivity_attention.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {\n",
    "    \"input\": [\"<sos>\", \"n\", \"e\", \"n\", \"u\", \"<eos>\"],\n",
    "    \"output\": [\"‡∞®\", \"‡±Ü\", \"‡∞®\", \"‡±Å\", \"<eos>\"],\n",
    "    \"attentions\": [\n",
    "        [0.01, 0.01, 0.01, 0.96],  # for \"‡∞®\"\n",
    "        [0.05, 0.02, 0.8, 0.13],   # for \"‡±Ü\"\n",
    "        [0.1, 0.6, 0.1, 0.2],      # etc.\n",
    "        [0.25, 0.25, 0.25, 0.25],\n",
    "        [0.5, 0.1, 0.2, 0.2]\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ HTML file saved: attention_viz_clip.html\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def write_attention_html_clip(\n",
    "    example, \n",
    "    filename=\"attention_viz_clip.html\", \n",
    "    clip_threshold=0.5, \n",
    "    top_k=None\n",
    "):\n",
    "    html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "  <meta charset=\"utf-8\">\n",
    "  <title>Attention Connectivity Visualization</title>\n",
    "  <style>\n",
    "    .tok {{\n",
    "      padding:4px 6px; margin:2px; border-radius:4px;\n",
    "      display:inline-block; font-family:monospace; background:#eee; cursor:pointer;\n",
    "      transition: background 0.2s; border: 2px solid transparent;\n",
    "    }}\n",
    "    .clip {{\n",
    "      border: 2px solid #005f00 !important;\n",
    "    }}\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "  <h3>Interactive Attention: Hover output ‚Üí see input focus</h3>\n",
    "  <div><b>Input:</b> <span id=\"in\"></span></div>\n",
    "  <div><b>Output:</b> <span id=\"out\"></span></div>\n",
    "  <script>\n",
    "    const data = {data_json};\n",
    "    const inDiv  = document.getElementById(\"in\");\n",
    "    const outDiv = document.getElementById(\"out\");\n",
    "    const shownInputs = data.input.slice(1, -1);\n",
    "    shownInputs.forEach(function(ch,i){{\n",
    "      const s=document.createElement(\"span\");\n",
    "      s.textContent=ch; s.className=\"tok\"; s.id=\"in-\"+i;\n",
    "      inDiv.appendChild(s);\n",
    "    }});\n",
    "    function resetInputColors() {{\n",
    "      shownInputs.forEach(function(_,j){{\n",
    "        var el = document.getElementById(\"in-\"+j);\n",
    "        el.style.background = \"#eee\";\n",
    "        el.classList.remove(\"clip\");\n",
    "      }});\n",
    "    }}\n",
    "    function attColor(v) {{\n",
    "      // v in [0,1]; HSL: 120=green, 0=saturated; L=90% (light)‚Üí40% (dark)\n",
    "      // From very light to dark green\n",
    "      const lightness = 90 - 50*v; // v=0:90, v=1:40\n",
    "      return `hsl(120, 70%, ${{lightness}}%)`;\n",
    "    }}\n",
    "    function topIndices(arr, k) {{\n",
    "      let idx = arr.map((v,i)=>[v,i]);\n",
    "      idx.sort((a,b)=>b[0]-a[0]);\n",
    "      return idx.slice(0, k).map(x=>x[1]);\n",
    "    }}\n",
    "    data.output.forEach(function(ch,i){{\n",
    "      const s=document.createElement(\"span\");\n",
    "      s.textContent=ch; s.className=\"tok\";\n",
    "      s.onmouseenter=function(){{\n",
    "        resetInputColors();\n",
    "        const att = data.attentions[i];\n",
    "        // compute top indices for clip\n",
    "        let clip_indices = [];\n",
    "        if ({top_k} != null) {{\n",
    "          clip_indices = topIndices(att, {top_k});\n",
    "        }} else {{\n",
    "          const m = Math.max(...att);\n",
    "          clip_indices = att.map((v,j)=>v>=({clip_threshold}*m)?j:null).filter(x=>x!==null);\n",
    "        }}\n",
    "        att.forEach(function(v,j){{\n",
    "          var el = document.getElementById(\"in-\"+j);\n",
    "          el.style.background = attColor(v);  // <--- use color gradient\n",
    "          el.classList.remove(\"clip\");\n",
    "        }});\n",
    "        clip_indices.forEach(function(j){{\n",
    "          document.getElementById(\"in-\"+j).classList.add(\"clip\");\n",
    "        }});\n",
    "      }};\n",
    "      s.onmouseleave=resetInputColors;\n",
    "      outDiv.appendChild(s);\n",
    "    }});\n",
    "  </script>\n",
    "</body>\n",
    "</html>\n",
    "    \"\"\".format(\n",
    "        data_json=json.dumps(example, ensure_ascii=False),\n",
    "        clip_threshold=clip_threshold,\n",
    "        top_k='null' if top_k is None else top_k,\n",
    "    )\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "    print(f\"‚úÖ HTML file saved: {filename}\")\n",
    "\n",
    "# Usage: \n",
    "write_attention_html_clip(example, filename=\"attention_viz_clip.html\", clip_threshold=0.5, top_k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to my_attention.html\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def save_attention_html(example, filename=\"attention.html\", threshold=0.2):\n",
    "    # Use double braces {{ }} for JS/CSS in Python f-string!\n",
    "    html = f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <style>\n",
    "      .tok {{padding:4px 6px;margin:2px;border-radius:4px;\n",
    "             display:inline-block;font-family:monospace;\n",
    "             background:#eee;cursor:pointer;transition: background 0.15s;}}\n",
    "      .hi {{background:rgba(0,200,0,.5);}}\n",
    "    </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h3>Interactive Attention (hover output ‚Üí see input focus)</h3>\n",
    "    <div><b>Input:</b> <span id=\"in\"></span></div><br>\n",
    "    <div><b>Output:</b> <span id=\"out\"></span></div>\n",
    "\n",
    "    <script>\n",
    "      const data = {json.dumps(example, ensure_ascii=False)};\n",
    "      const threshold = {threshold};\n",
    "\n",
    "      // Use only visible input tokens (skip <sos>/<eos>)\n",
    "      const shownInputs = data.input.slice(1, -1);\n",
    "      const inDiv  = document.getElementById(\"in\");\n",
    "      const outDiv = document.getElementById(\"out\");\n",
    "\n",
    "      // Helper for gradient color: higher attention = deeper green\n",
    "      function attColor(a) {{\n",
    "        // a ‚àà [0,1], interpolate between white and green\n",
    "        const g = Math.round(255 - 180 * a);\n",
    "        return `rgb(${{g}},255,${{g}})`;\n",
    "      }}\n",
    "\n",
    "      // Draw input tokens (skip <sos>/<eos>)\n",
    "      shownInputs.forEach((ch,i)=>{{\n",
    "        const s=document.createElement(\"span\");\n",
    "        s.textContent=ch; s.className=\"tok\"; s.id=\"in-\"+i;\n",
    "        inDiv.appendChild(s);\n",
    "      }});\n",
    "\n",
    "      // Draw output tokens\n",
    "      data.output.forEach((ch,i)=>{{\n",
    "        const s=document.createElement(\"span\");\n",
    "        s.textContent=ch; s.className=\"tok\";\n",
    "\n",
    "        s.onmouseenter=()=>{{\n",
    "          // clear previous\n",
    "          shownInputs.forEach((_,j)=>{{\n",
    "            const el = document.getElementById(\"in-\"+j);\n",
    "            el.style.background = \"#eee\";\n",
    "          }});\n",
    "          // get attention vector for output token\n",
    "          const att = data.attentions[i] || [];\n",
    "          att.forEach((v,j)=>{{\n",
    "            const el = document.getElementById(\"in-\"+j);\n",
    "            if (v >= threshold) {{\n",
    "              el.style.background = attColor(v);\n",
    "            }}\n",
    "          }});\n",
    "        }};\n",
    "        s.onmouseleave=()=>{{\n",
    "          shownInputs.forEach((_,j)=>{{\n",
    "            document.getElementById(\"in-\"+j).style.background = \"#eee\";\n",
    "          }});\n",
    "        }};\n",
    "        outDiv.appendChild(s);\n",
    "      }});\n",
    "    </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "    print(f\"Saved to {filename}\")\n",
    "\n",
    "# Example usage:\n",
    "save_attention_html(example, \"my_attention.html\", threshold=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded LSTM, GRU, and RNN models successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sathwikpentela/miniforge3/envs/PINN/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# --- Configs used for all models ---\n",
    "model_configs = {\n",
    "    \"lstm\": {\n",
    "        \"embedding_dim\": 256,\n",
    "        \"hidden_dim\": 256,\n",
    "        \"dropout\": 0.2,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"checkpoint\": \"best_model_attn (2).pth\"  # Change to your file path\n",
    "    },\n",
    "    \"gru\": {\n",
    "        \"embedding_dim\": 256,\n",
    "        \"hidden_dim\": 256,\n",
    "        \"dropout\": 0.4,\n",
    "        \"cell_type\": \"GRU\",\n",
    "        \"checkpoint\": \"best_model_attn_GRU.pth\"\n",
    "    },\n",
    "    \"rnn\": {\n",
    "        \"embedding_dim\": 256,\n",
    "        \"hidden_dim\": 256,\n",
    "        \"dropout\": 0.2,\n",
    "        \"cell_type\": \"RNN\",\n",
    "        \"checkpoint\": \"best_model_attn_RNN.pth\"\n",
    "    }\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "models = {}\n",
    "\n",
    "for model_type, cfg in model_configs.items():\n",
    "    encoder = AttentionEncoder(\n",
    "        vocab_size=len(input_vocab),\n",
    "        embedding_dim=cfg[\"embedding_dim\"],\n",
    "        hidden_dim=cfg[\"hidden_dim\"],\n",
    "        dropout=cfg[\"dropout\"],\n",
    "        cell_type=cfg[\"cell_type\"]\n",
    "    )\n",
    "    decoder = AttentionDecoder(\n",
    "        vocab_size=len(target_vocab),\n",
    "        embedding_dim=cfg[\"embedding_dim\"],\n",
    "        enc_hidden_dim=cfg[\"hidden_dim\"],\n",
    "        dec_hidden_dim=cfg[\"hidden_dim\"],\n",
    "        dropout=cfg[\"dropout\"],\n",
    "        cell_type=cfg[\"cell_type\"]\n",
    "    )\n",
    "    model = AttentionSeq2Seq(encoder, decoder, device).to(device)\n",
    "    model.load_state_dict(torch.load(cfg[\"checkpoint\"], map_location=device))\n",
    "    model.eval()\n",
    "    models[model_type] = model\n",
    "\n",
    "print(\"‚úÖ Loaded LSTM, GRU, and RNN models successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_attention_example(model, input_text, input2idx, target2idx, idx2target, device, max_len=30):\n",
    "    # Prepare input tensor (with <sos> and <eos>)\n",
    "    src_tensor = torch.tensor([tokenize(input_text, input2idx, add_sos_eos=True)], device=device)\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "    input_tokens = [SOS_TOKEN] + list(input_text) + [EOS_TOKEN]\n",
    "    output_tokens, attentions = [], []\n",
    "\n",
    "    dec_input = torch.tensor([target2idx[SOS_TOKEN]], device=device)\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output, hidden, attn_weights = model.decoder(dec_input, hidden, encoder_outputs)\n",
    "            top1 = output.argmax(1).item()\n",
    "            output_tokens.append(idx2target[top1])\n",
    "            attentions.append(attn_weights.squeeze(0).cpu().numpy().tolist())\n",
    "        if idx2target[top1] == EOS_TOKEN:\n",
    "            break\n",
    "        dec_input = torch.tensor([top1], device=device)\n",
    "    return {\n",
    "        \"input\": input_tokens,\n",
    "        \"output\": output_tokens,\n",
    "        \"attentions\": attentions\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_attention_html(example, filename=\"attention.html\", threshold=0.2):\n",
    "    html_template = \"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <style>\n",
    "      .tok {{padding:4px 6px;margin:2px;border-radius:4px;\n",
    "             display:inline-block;font-family:monospace;\n",
    "             background:#eee;cursor:pointer;transition: background 0.15s;}}\n",
    "    </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h3>Interactive Attention (hover output ‚Üí see input focus)</h3>\n",
    "    <div><b>Input:</b> <span id=\"in\"></span></div><br>\n",
    "    <div><b>Output:</b> <span id=\"out\"></span></div>\n",
    "    <script>\n",
    "      const data = {json_data};\n",
    "      const threshold = {threshold};\n",
    "\n",
    "      const shownInputs = data.input.slice(1, -1);  // Skip <sos> and <eos>\n",
    "      const inDiv  = document.getElementById(\"in\");\n",
    "      const outDiv = document.getElementById(\"out\");\n",
    "\n",
    "      function attColor(a) {{\n",
    "        const g = Math.round(255 - 180 * a);\n",
    "        return `rgb(${{g}},255,${{g}})`;\n",
    "      }}\n",
    "\n",
    "      shownInputs.forEach((ch,i)=> {{\n",
    "        const s = document.createElement(\"span\");\n",
    "        s.textContent = ch;\n",
    "        s.className = \"tok\";\n",
    "        s.id = \"in-\" + i;\n",
    "        inDiv.appendChild(s);\n",
    "      }});\n",
    "\n",
    "      data.output.forEach((ch, i) => {{\n",
    "        const s = document.createElement(\"span\");\n",
    "        s.textContent = ch;\n",
    "        s.className = \"tok\";\n",
    "\n",
    "        s.onmouseenter = () => {{\n",
    "          shownInputs.forEach((_, j) => {{\n",
    "            const el = document.getElementById(\"in-\" + j);\n",
    "            el.style.background = \"#eee\";\n",
    "          }});\n",
    "          const att = data.attentions[i] || [];\n",
    "          att.forEach((v, j) => {{\n",
    "            if (v >= threshold) {{\n",
    "              document.getElementById(\"in-\" + j).style.background = attColor(v);\n",
    "            }}\n",
    "          }});\n",
    "        }};\n",
    "\n",
    "        s.onmouseleave = () => {{\n",
    "          shownInputs.forEach((_, j) => {{\n",
    "            document.getElementById(\"in-\" + j).style.background = \"#eee\";\n",
    "          }});\n",
    "        }};\n",
    "\n",
    "        outDiv.appendChild(s);\n",
    "      }});\n",
    "    </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    html = html_template.format(\n",
    "        json_data=json.dumps(example, ensure_ascii=False),\n",
    "        threshold=threshold\n",
    "    )\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "    print(f\"‚úÖ HTML saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ HTML saved to attention_lstm.html\n",
      "‚úÖ HTML saved to attention_gru.html\n",
      "‚úÖ HTML saved to attention_rnn.html\n"
     ]
    }
   ],
   "source": [
    "test_word = \"sitaramayya\"  # or any test word\n",
    "\n",
    "for name, model in models.items():\n",
    "    ex = get_attention_example(model, test_word, input2idx, target2idx, idx2target, device)\n",
    "    fname = f\"attention_{name}.html\"\n",
    "    save_attention_html(ex, fname, threshold=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_multiple_attention_html(model_examples, filename=\"attention_all_models.html\", threshold=0.15):\n",
    "    html_blocks = []\n",
    "\n",
    "    for name, example in model_examples.items():\n",
    "        block = f\"\"\"\n",
    "        <h3 style='margin-top:30px;'>Model: {name.upper()}</h3>\n",
    "        <div><b>Input:</b> <span id=\"in-{name}\"></span></div><br>\n",
    "        <div><b>Output:</b> <span id=\"out-{name}\"></span></div>\n",
    "        <script>\n",
    "          const data_{name} = {json.dumps(example, ensure_ascii=False)};\n",
    "          const threshold_{name} = {threshold};\n",
    "\n",
    "          const shownInputs_{name} = data_{name}.input.slice(1, -1);\n",
    "          const inDiv_{name}  = document.getElementById(\"in-{name}\");\n",
    "          const outDiv_{name} = document.getElementById(\"out-{name}\");\n",
    "\n",
    "          function attColor_{name}(a) {{\n",
    "            const g = Math.round(255 - 180 * a);\n",
    "            return `rgb(${{g}},255,${{g}})`;\n",
    "          }}\n",
    "\n",
    "          shownInputs_{name}.forEach((ch,i)=> {{\n",
    "            const s=document.createElement(\"span\");\n",
    "            s.textContent=ch; s.className=\"tok\"; s.id=\"in-{name}-\"+i;\n",
    "            inDiv_{name}.appendChild(s);\n",
    "          }});\n",
    "\n",
    "          data_{name}.output.forEach((ch,i)=> {{\n",
    "            const s=document.createElement(\"span\");\n",
    "            s.textContent=ch; s.className=\"tok\";\n",
    "\n",
    "            s.onmouseenter=()=> {{\n",
    "              shownInputs_{name}.forEach((_,j)=> {{\n",
    "                const el = document.getElementById(\"in-{name}-\"+j);\n",
    "                el.style.background = \"#eee\";\n",
    "              }});\n",
    "              const att = data_{name}.attentions[i] || [];\n",
    "              att.forEach((v,j)=> {{\n",
    "                if (v >= threshold_{name}) {{\n",
    "                  document.getElementById(\"in-{name}-\"+j).style.background = attColor_{name}(v);\n",
    "                }}\n",
    "              }});\n",
    "            }};\n",
    "            s.onmouseleave=()=> {{\n",
    "              shownInputs_{name}.forEach((_,j)=> {{\n",
    "                document.getElementById(\"in-{name}-\"+j).style.background = \"#eee\";\n",
    "              }});\n",
    "            }};\n",
    "            outDiv_{name}.appendChild(s);\n",
    "          }});\n",
    "        </script>\n",
    "        \"\"\"\n",
    "        html_blocks.append(block)\n",
    "\n",
    "    full_html = f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <style>\n",
    "      .tok {{\n",
    "        padding:4px 6px;\n",
    "        margin:2px;\n",
    "        border-radius:4px;\n",
    "        display:inline-block;\n",
    "        font-family:monospace;\n",
    "        background:#eee;\n",
    "        cursor:pointer;\n",
    "        transition: background 0.15s;\n",
    "      }}\n",
    "    </style>\n",
    "    </head>\n",
    "    <body>\n",
    "      <h2>Interactive Attention Visualization for Multiple Models</h2>\n",
    "      {''.join(html_blocks)}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_html)\n",
    "\n",
    "    print(f\"‚úÖ Combined attention HTML saved to: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Combined attention HTML saved to: attention_all_models.html\n"
     ]
    }
   ],
   "source": [
    "# Define the test word\n",
    "test_word = \"sitaramayya\"\n",
    "\n",
    "# Generate examples for each model\n",
    "model_examples = {}\n",
    "for name, model in models.items():\n",
    "    example = get_attention_example(model, test_word, input2idx, target2idx, idx2target, device)\n",
    "    model_examples[name] = example\n",
    "\n",
    "# Save all attention visualizations in one file\n",
    "save_multiple_attention_html(model_examples, filename=\"attention_all_models.html\", threshold=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/sathwikpentela/MTECH/DL/Assignment 3/DA6401_Assignment_3/wandb/run-20250521_223210-w8i00sdl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m017-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/w8i00sdl' target=\"_blank\">interactive_attention</a></strong> to <a href='https://wandb.ai/da24m017-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m017-indian-institute-of-technology-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/da24m017-indian-institute-of-technology-madras/DA6401_Assignment_3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m017-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/w8i00sdl' target=\"_blank\">https://wandb.ai/da24m017-indian-institute-of-technology-madras/DA6401_Assignment_3/runs/w8i00sdl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Artifact attention_visualization>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Start or resume a W&B run\n",
    "wandb.init(project=\"DA6401_Assignment_3\", name=\"interactive_attention\")\n",
    "\n",
    "# Log the HTML file\n",
    "artifact = wandb.Artifact(\"attention_visualization\", type=\"visualization\")\n",
    "artifact.add_file(\"attention_all_models.html\")\n",
    "wandb.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('da24m017-indian-institute-of-technology-madras/DA6401_Assignment_3/attention_visualization:v0', type='visualization')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_example(model, input_text, input2idx, target2idx, idx2target, device, max_len=30):\n",
    "    src_tensor = torch.tensor([tokenize(input_text, input2idx, add_sos_eos=True)], device=device)\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "    input_tokens = [SOS_TOKEN] + list(input_text) + [EOS_TOKEN]\n",
    "    output_tokens, attentions = [], []\n",
    "\n",
    "    dec_input = torch.tensor([target2idx[SOS_TOKEN]], device=device)\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output, hidden, attn_weights = model.decoder(dec_input, hidden, encoder_outputs)\n",
    "            top1 = output.argmax(1).item()\n",
    "            output_tokens.append(idx2target[top1])\n",
    "            attentions.append(attn_weights.squeeze(0).cpu().numpy().tolist())\n",
    "        if idx2target[top1] == EOS_TOKEN:\n",
    "            break\n",
    "        dec_input = torch.tensor([top1], device=device)\n",
    "    return {\n",
    "        \"input\": input_tokens,\n",
    "        \"output\": output_tokens,\n",
    "        \"attentions\": attentions\n",
    "    }\n",
    "\n",
    "def collect_attention_data_for_words(models, word_list):\n",
    "    results = {}\n",
    "    for i, word in enumerate(word_list):\n",
    "        results[f\"Sample {i+1}\"] = {\n",
    "            \"input\": word,\n",
    "            \"models\": {\n",
    "                name: get_attention_example(model, word, input2idx, target2idx, idx2target, device)\n",
    "                for name, model in models.items()\n",
    "            }\n",
    "        }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def collect_attention_data_for_words(models, word_list):\n",
    "    results = {}\n",
    "    for i, word in enumerate(word_list):\n",
    "        results[f\"Sample {i+1}\"] = {\n",
    "            \"input\": word,\n",
    "            \"models\": {\n",
    "                name: get_attention_example(model, word, input2idx, target2idx, idx2target, device)\n",
    "                for name, model in models.items()\n",
    "            }\n",
    "        }\n",
    "    return results\n",
    "\n",
    "def save_multiword_attention_html(examples_dict, filename=\"multiword_attention.html\", threshold=0.15):\n",
    "    blocks = []\n",
    "    for sample_name, sample in examples_dict.items():\n",
    "        input_word = sample[\"input\"]\n",
    "        blocks.append(f\"<h3>{sample_name}</h3><div><b>Input (Latin):</b> {input_word}</div><br>\")\n",
    "        for model_name, data in sample[\"models\"].items():\n",
    "            block = f\"\"\"\n",
    "            <div><b>Model: {model_name.upper()}</b></div>\n",
    "            <div><b>Output (Telugu):</b> <span id=\"{sample_name}_{model_name}_out\"></span></div>\n",
    "            <script>\n",
    "              const data_{sample_name}_{model_name} = {json.dumps(data, ensure_ascii=False)};\n",
    "              const threshold = {threshold};\n",
    "\n",
    "              const outDiv = document.getElementById(\"{sample_name}_{model_name}_out\");\n",
    "              data_{sample_name}_{model_name}.output.forEach((ch, i) => {{\n",
    "                const s = document.createElement(\"span\");\n",
    "                s.textContent = ch;\n",
    "                s.className = \"tok\";\n",
    "                s.title = data_{sample_name}_{model_name}.input[\n",
    "                  data_{sample_name}_{model_name}.attentions[i]?.indexOf(\n",
    "                    Math.max(...data_{sample_name}_{model_name}.attentions[i])\n",
    "                  )\n",
    "                ] || '';\n",
    "                outDiv.appendChild(s);\n",
    "              }});\n",
    "            </script><br><br>\n",
    "            \"\"\"\n",
    "            blocks.append(block)\n",
    "\n",
    "    full_html = f\"\"\"\n",
    "    <html>\n",
    "    <head><meta charset=\"UTF-8\">\n",
    "    <style>.tok {{\n",
    "        padding: 6px;\n",
    "        background: #673ab7;\n",
    "        color: white;\n",
    "        margin: 2px;\n",
    "        border-radius: 4px;\n",
    "        display: inline-block;\n",
    "        font-family: monospace;\n",
    "    }}</style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h2>d3js_attention_visualization</h2>\n",
    "        {''.join(blocks)}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_html)\n",
    "    print(f\"‚úÖ HTML saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ HTML saved to multiword_attention.html\n"
     ]
    }
   ],
   "source": [
    "# List of test inputs to visualize\n",
    "sample_words = [\"antamavutundi\", \"oravadi\", \"kaavatamtho\", \"ghoramgaa\", \"yuvataku\"]\n",
    "\n",
    "# Collect model outputs + attention weights\n",
    "multiword_examples = collect_attention_data_for_words(models, sample_words)\n",
    "\n",
    "# Save to HTML\n",
    "save_multiword_attention_html(multiword_examples, filename=\"multiword_attention.html\", threshold=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, torch, os, math, html\n",
    "\n",
    "# ---------- utility -------------\n",
    "def safe(s):          # escape <, >, & for proper HTML\n",
    "    return html.escape(str(s), quote=False)\n",
    "\n",
    "def get_attention_example(model, input_text,\n",
    "                          input2idx, target2idx, idx2target,\n",
    "                          device, max_len=30):\n",
    "    \"\"\"Return dict {input, output, attentions} (list-of-lists).\"\"\"\n",
    "    src = torch.tensor([ [input2idx[SOS_TOKEN]] +\n",
    "                         [input2idx[c] for c in input_text] +\n",
    "                         [input2idx[EOS_TOKEN]] ],\n",
    "                       device=device)\n",
    "    enc_out, hidden = model.encoder(src)\n",
    "    input_tokens  = [SOS_TOKEN] + list(input_text) + [EOS_TOKEN]\n",
    "    output_tokens, att_wts = [], []\n",
    "\n",
    "    dec_in = torch.tensor([target2idx[SOS_TOKEN]], device=device)\n",
    "    for _ in range(max_len):\n",
    "        out, hidden, att = model.decoder(dec_in, hidden, enc_out)\n",
    "        top = out.argmax(1).item()\n",
    "        output_tokens.append(idx2target[top])\n",
    "        att_wts.append(att.squeeze(0).tolist())\n",
    "        if idx2target[top] == EOS_TOKEN: break\n",
    "        dec_in = torch.tensor([top], device=device)\n",
    "\n",
    "    return {\"input\":input_tokens, \"output\":output_tokens,\"att\":att_wts}\n",
    "\n",
    "def collect_examples(models:dict, words:list):\n",
    "    \"\"\"Return {sample#:{input:str, models:{name:example}}}\"\"\"\n",
    "    res={}\n",
    "    for i, w in enumerate(words):\n",
    "        res[f\"Sample {i+1}\"] = {\n",
    "            \"input\": w,\n",
    "            \"models\": {name:get_attention_example(m,w,input2idx,\n",
    "                                                  target2idx,idx2target,device)\n",
    "                       for name,m in models.items()}\n",
    "        }\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved interactive HTML: multiword_attention.html\n"
     ]
    }
   ],
   "source": [
    "###########  parameters  ###################\n",
    "WORDS   = [\"antamavutundi\", \"oravadi\", \"kaavatamtho\", \"ghoramgaa\", \"yuvataku\"]\n",
    "OUT_FN  = \"multiword_attention.html\"\n",
    "TH      = 0.15                                # highlight threshold\n",
    "#############################################\n",
    "\n",
    "# 1. gather data\n",
    "examples = collect_examples(models, WORDS)\n",
    "\n",
    "# 2. build HTML  ‚Äì use placeholder tokens then .replace()\n",
    "DATA_JS = json.dumps(examples, ensure_ascii=False)\n",
    "template = r\"\"\"\n",
    "<!DOCTYPE html><html><head><meta charset=\"utf-8\">\n",
    "<title>Attention Visualizer</title>\n",
    "<style>\n",
    " body {font-family:Inter, sans-serif;background:#f5f6fa}\n",
    " h2  {text-align:center;color:#4b32c3}\n",
    " .card{display:inline-block;vertical-align:top;margin:16px;\n",
    "       padding:16px 20px;border-radius:8px;\n",
    "       background:#fff;box-shadow:0 1px 4px rgba(0,0,0,.12)}\n",
    " .tok {display:inline-block;padding:6px 8px;\n",
    "       margin:2px;border-radius:4px;\n",
    "       background:#673ab7;color:#fff;font-family:monospace;\n",
    "       cursor:pointer;transition:background .15s}\n",
    " .inputtok{background:#eee;color:#111}\n",
    "</style></head><body>\n",
    "<h2>Interactive Attention Visualization for Multiple Models</h2>\n",
    "<div id=\"root\"></div>\n",
    "\n",
    "<script>\n",
    "const DATA = __DATA_PLACEHOLDER__;   // injected JSON\n",
    "const TH   = __THRESHOLD_PLACEHOLDER__;\n",
    "\n",
    "function attColor(a){            // greenish highlight\n",
    "  const g = Math.round(255 - 180*a);\n",
    "  return `rgb(${g},255,${g})`;\n",
    "}\n",
    "\n",
    "const root = document.getElementById(\"root\");\n",
    "\n",
    "Object.entries(DATA).forEach(([sname,sample])=>{\n",
    "  const div = document.createElement(\"div\");\n",
    "  div.innerHTML = `<h3>${sname}</h3>\n",
    "                   <p><b>Input (Latin):</b> ${sample.input}</p>`;\n",
    "  Object.entries(sample.models).forEach(([mname,ex])=>{\n",
    "    const c = document.createElement(\"div\"); c.className=\"card\";\n",
    "\n",
    "    // input row\n",
    "    const inRow = document.createElement(\"div\");\n",
    "    ex.input.slice(1,-1).forEach((ch,i)=>{\n",
    "       const sp = document.createElement(\"span\");\n",
    "       sp.textContent = ch; sp.className = \"tok inputtok\";\n",
    "       sp.id = `${sname}_${mname}_in_${i}`;\n",
    "       inRow.appendChild(sp);\n",
    "    });\n",
    "\n",
    "    // output row\n",
    "    const outRow = document.createElement(\"div\");\n",
    "    ex.output.forEach((ch,oi)=>{\n",
    "       const sp = document.createElement(\"span\");\n",
    "       sp.textContent = ch; sp.className = \"tok\";\n",
    "       sp.onmouseenter = ()=>{\n",
    "         ex.att[oi]?.forEach((v,ii)=>{\n",
    "           const el = document.getElementById(`${sname}_${mname}_in_${ii}`);\n",
    "           if (el) el.style.background = v >= TH ? attColor(v) : \"#eee\";\n",
    "         });\n",
    "       };\n",
    "       sp.onmouseleave = ()=>{\n",
    "         ex.input.slice(1,-1).forEach((_,ii)=>{\n",
    "           const el = document.getElementById(`${sname}_${mname}_in_${ii}`);\n",
    "           if (el) el.style.background = \"#eee\";\n",
    "         });\n",
    "       };\n",
    "       outRow.appendChild(sp);\n",
    "    });\n",
    "\n",
    "    c.innerHTML = `<h4>Model: ${mname.toUpperCase()}</h4>`;\n",
    "    c.appendChild(inRow); c.appendChild(document.createElement(\"br\"));\n",
    "    c.appendChild(outRow); div.appendChild(c);\n",
    "  });\n",
    "  root.appendChild(div);\n",
    "});\n",
    "</script></body></html>\n",
    "\"\"\"\n",
    "\n",
    "# Replace placeholders safely (only two spots)\n",
    "html_doc = (template\n",
    "            .replace(\"__DATA_PLACEHOLDER__\", DATA_JS)\n",
    "            .replace(\"__THRESHOLD_PLACEHOLDER__\", str(TH)))\n",
    "\n",
    "with open(OUT_FN, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html_doc)\n",
    "\n",
    "print(\"‚úÖ Saved interactive HTML:\", OUT_FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"DA6401_Assignment_3\", name=\"multiword_attn_vis\") \n",
    "wandb.log({\"multiword_attention_html\": wandb.Html(\"multiword_attention.html\")})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7436399,
     "sourceId": 11836481,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7461942,
     "sourceId": 11873573,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7461947,
     "sourceId": 11873579,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7462531,
     "sourceId": 11874388,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 350127,
     "modelInstanceId": 329310,
     "sourceId": 402635,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 351259,
     "modelInstanceId": 330421,
     "sourceId": 404221,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "PINN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
